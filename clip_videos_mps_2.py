import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# å¿½ç•¥NumPyè­¦å‘Š
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)

import torch
import torch.nn.functional as F
from PIL import Image
import numpy as np
from pathlib import Path
import json
import cv2

# ç›´æ¥å¯¼å…¥æ ¸å¿ƒç»„ä»¶ï¼Œç»•è¿‡image_processingé—®é¢˜
def load_clip_directly():
    """ç›´æ¥åŠ è½½CLIPæ¨¡å‹ï¼Œç»•è¿‡image_processingé—®é¢˜"""
    try:
        # ç›´æ¥ä»modelingæ¨¡å—å¯¼å…¥ï¼Œé¿å…processingé—®é¢˜
        from transformers.models.clip.modeling_clip import CLIPModel
        from transformers.models.clip.configuration_clip import CLIPConfig
        from transformers import AutoTokenizer

        print("âœ… æˆåŠŸå¯¼å…¥æ ¸å¿ƒCLIPç»„ä»¶")
        return CLIPModel, CLIPConfig, AutoTokenizer, True
    except ImportError as e:
        print(f"âŒ æ ¸å¿ƒç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
        return None, None, None, False

# åˆ›å»ºç®€åŒ–çš„å¤„ç†å™¨
class SimpleProcessor:
    """ç®€åŒ–çš„CLIPå¤„ç†å™¨ï¼Œé¿å…image_processingé—®é¢˜"""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, text=None, images=None, return_tensors="pt", padding=True):
        # å¤„ç†æ–‡æœ¬
        if text is not None:
            if isinstance(text, str):
                text = [text]

            # ä½¿ç”¨tokenizerå¤„ç†æ–‡æœ¬
            text_inputs = self.tokenizer(
                text,
                return_tensors=return_tensors,
                padding=padding,
                truncation=True,
                max_length=77
            )
        else:
            text_inputs = {}

        # å¤„ç†å›¾åƒ
        if images is not None:
            if isinstance(images, Image.Image):
                images = [images]

            # ç®€å•çš„å›¾åƒé¢„å¤„ç†
            pixel_values = []
            for img in images:
                # è°ƒæ•´å¤§å°åˆ°224x224
                img = img.resize((224, 224))
                # è½¬æ¢ä¸ºtensor
                img_array = torch.tensor(list(img.getdata())).float()
                img_array = img_array.view(224, 224, 3)
                # è½¬æ¢ä¸ºCHWæ ¼å¼å¹¶å½’ä¸€åŒ–
                img_tensor = img_array.permute(2, 0, 1) / 255.0
                # æ ‡å‡†åŒ– (ImageNetæ ‡å‡†)
                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                img_tensor = (img_tensor - mean) / std
                pixel_values.append(img_tensor)

            pixel_values = torch.stack(pixel_values)
            text_inputs['pixel_values'] = pixel_values

        return text_inputs

# å°è¯•å¯¼å…¥ç»„ä»¶
CLIPModel, CLIPConfig, AutoTokenizer, clip_available = load_clip_directly()

# è®¾ç½®è®¾å¤‡ - ä¼˜å…ˆä½¿ç”¨MPS
def get_device():
    if torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")

device = get_device()
print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")

class VideoAnalyzer:
    """è§†é¢‘åˆ†æå™¨ - è¯»å–çœŸå®è§†é¢‘æ–‡ä»¶å¹¶è¿›è¡ŒCLIPåˆ†æ"""

    def __init__(self):
        self.model_loaded = False
        self._load_clip_model()

    def _load_clip_model(self):
        """åŠ è½½CLIPæ¨¡å‹"""
        if not clip_available:
            print("âŒ CLIPç»„ä»¶ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
            self._create_demo_model()
            return

        try:
            print("ğŸ“¦ åŠ è½½CLIPæ¨¡å‹...")

            # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶
            model_dir = Path("./models")
            possible_paths = [
                model_dir / "models--openai--clip-vit-base-patch32" / "snapshots",
                model_dir / "clip-model",
                model_dir
            ]

            local_model_path = None
            for path in possible_paths:
                if path.exists():
                    # æŸ¥æ‰¾åŒ…å«config.jsonçš„å­ç›®å½•
                    for subdir in path.rglob("*"):
                        if subdir.is_dir() and (subdir / "config.json").exists():
                            local_model_path = subdir
                            break
                    if local_model_path:
                        break

            if local_model_path:
                print(f"ğŸ“ æ‰¾åˆ°æœ¬åœ°æ¨¡å‹æ–‡ä»¶: {local_model_path}")

                # åŠ è½½é…ç½®
                config_path = local_model_path / "config.json"
                with open(config_path, 'r') as f:
                    config_dict = json.load(f)
                config = CLIPConfig.from_dict(config_dict)

                # åŠ è½½æ¨¡å‹
                self.model = CLIPModel.from_pretrained(
                    str(local_model_path),
                    config=config,
                    local_files_only=True
                ).to(device)

                # åŠ è½½tokenizer
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        str(local_model_path),
                        local_files_only=True
                    )
                except:
                    print("âš ï¸  æœ¬åœ°tokenizeråŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åœ¨çº¿ç‰ˆæœ¬...")
                    self.tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

                # åˆ›å»ºç®€åŒ–å¤„ç†å™¨
                self.processor = SimpleProcessor(self.tokenizer)

                print(f"âœ… CLIPæ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¿è¡Œåœ¨ {device}")
                self.model_loaded = True

            else:
                print("âŒ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹æ–‡ä»¶")
                raise Exception("æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")

        except Exception as e:
            print(f"âŒ CLIPæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”§ ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼...")
            self.model_loaded = False
            self._create_demo_model()

    def _create_demo_model(self):
        """åˆ›å»ºæ¼”ç¤ºæ¨¡å‹"""
        class DemoModel:
            def __call__(self, **kwargs):
                batch_size = kwargs.get('pixel_values', torch.randn(1, 3, 224, 224)).shape[0]
                text_batch_size = kwargs.get('input_ids', torch.randint(0, 1000, (1, 77))).shape[0]

                return type('Output', (), {
                    'image_embeds': torch.randn(batch_size, 512),
                    'text_embeds': torch.randn(text_batch_size, 512),
                    'logits_per_image': torch.randn(batch_size, text_batch_size),
                    'logits_per_text': torch.randn(text_batch_size, batch_size)
                })()

        class DemoProcessor:
            def __call__(self, images=None, text=None, return_tensors="pt", padding=True):
                batch_size = len(text) if text else 1
                return {
                    'pixel_values': torch.randn(1, 3, 224, 224),
                    'input_ids': torch.randint(0, 1000, (batch_size, 77)),
                    'attention_mask': torch.ones(batch_size, 77)
                }

        self.model = DemoModel()
        self.processor = DemoProcessor()
        print("âœ… æ¼”ç¤ºæ¨¡å‹åˆ›å»ºå®Œæˆ")

    def extract_frames_from_video(self, video_path, max_frames=30, sample_rate=1):
        """
        ä»è§†é¢‘æ–‡ä»¶ä¸­æå–å¸§

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            max_frames: æœ€å¤§æå–å¸§æ•°
            sample_rate: é‡‡æ ·ç‡ï¼ˆæ¯sample_rateå¸§å–ä¸€å¸§ï¼‰

        Returns:
            frames: PILå›¾åƒåˆ—è¡¨
            frame_info: å¸§ä¿¡æ¯ï¼ˆæ—¶é—´æˆ³ç­‰ï¼‰
        """
        print(f"ğŸï¸ ä»è§†é¢‘æå–å¸§: {video_path}")

        if not Path(video_path).exists():
            print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return [], []

        cap = cv2.VideoCapture(str(video_path))

        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return [], []

        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0

        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯:")
        print(f"   - æ€»å¸§æ•°: {total_frames}")
        print(f"   - å¸§ç‡: {fps:.2f} FPS")
        print(f"   - æ—¶é•¿: {duration:.2f} ç§’")

        frames = []
        frame_info = []
        frame_count = 0
        extracted_count = 0

        while cap.isOpened() and extracted_count < max_frames:
            ret, frame = cap.read()
            if not ret:
                break

            # æŒ‰é‡‡æ ·ç‡æå–å¸§
            if frame_count % sample_rate == 0:
                # è½¬æ¢BGRåˆ°RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # è½¬æ¢ä¸ºPILå›¾åƒ
                pil_frame = Image.fromarray(frame_rgb)
                frames.append(pil_frame)

                # è®°å½•å¸§ä¿¡æ¯
                timestamp = frame_count / fps if fps > 0 else frame_count
                frame_info.append({
                    'frame_number': frame_count,
                    'timestamp': timestamp,
                    'extracted_index': extracted_count
                })
                extracted_count += 1

            frame_count += 1

        cap.release()
        print(f"âœ… æå–äº† {len(frames)} å¸§")
        return frames, frame_info

    def load_text_queries(self, text_path):
        """
        ä»æ–‡æœ¬æ–‡ä»¶åŠ è½½æŸ¥è¯¢æ–‡æœ¬

        Args:
            text_path: æ–‡æœ¬æ–‡ä»¶è·¯å¾„

        Returns:
            texts: æ–‡æœ¬åˆ—è¡¨
        """
        print(f"ğŸ“ åŠ è½½æ–‡æœ¬æŸ¥è¯¢: {text_path}")

        if not Path(text_path).exists():
            print(f"âŒ æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {text_path}")
            return []

        texts = []
        try:
            with open(text_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):  # å¿½ç•¥ç©ºè¡Œå’Œæ³¨é‡Š
                        texts.append(line)

            print(f"âœ… åŠ è½½äº† {len(texts)} ä¸ªæ–‡æœ¬æŸ¥è¯¢:")
            for i, text in enumerate(texts, 1):
                print(f"   {i}. {text}")

            return texts

        except Exception as e:
            print(f"âŒ è¯»å–æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
            return []

    def analyze_frame(self, frame, texts):
        """åˆ†æå•ä¸ªå¸§"""
        inputs = self.processor(
            images=frame,
            text=texts,
            return_tensors="pt",
            padding=True
        )

        if self.model_loaded:
            # å°†è¾“å…¥ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                     for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = F.softmax(outputs.logits_per_image, dim=-1)

        return probs.cpu() if self.model_loaded else probs

    def analyze_video(self, video_path, text_path, output_dir):
        """
        åˆ†æè§†é¢‘æ–‡ä»¶

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            text_path: æ–‡æœ¬æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        print(f"\nğŸ¬ å¼€å§‹åˆ†æè§†é¢‘")
        print("=" * 60)

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # 1. æå–è§†é¢‘å¸§
        frames, frame_info = self.extract_frames_from_video(video_path, max_frames=50, sample_rate=5)
        if not frames:
            print("âŒ æ— æ³•æå–è§†é¢‘å¸§")
            return False

        # 2. åŠ è½½æ–‡æœ¬æŸ¥è¯¢
        texts = self.load_text_queries(text_path)
        if not texts:
            print("âŒ æ— æ³•åŠ è½½æ–‡æœ¬æŸ¥è¯¢")
            return False

        # 3. åˆ†ææ¯ä¸€å¸§
        print(f"\nğŸ” å¼€å§‹CLIPåˆ†æ...")
        results = []

        for i, (frame, info) in enumerate(zip(frames, frame_info)):
            print(f"   åˆ†æå¸§ {i+1}/{len(frames)} (æ—¶é—´: {info['timestamp']:.2f}s)")
            probs = self.analyze_frame(frame, texts)
            results.append(probs[0])  # å–ç¬¬ä¸€ä¸ªå›¾åƒçš„ç»“æœ

        results = torch.stack(results)

        # 4. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self._generate_analysis_report(
            video_path, text_path, texts, results, frame_info, output_path
        )

        # 5. åˆ›å»ºå¯è§†åŒ–è§†é¢‘
        self._create_analysis_video(
            frames, texts, results, frame_info, output_path / "analysis_video.mp4"
        )

        # 6. ä¿å­˜å…³é”®å¸§
        self._save_key_frames(frames, texts, results, frame_info, output_path)

        print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_path}")
        return True

    def _generate_analysis_report(self, video_path, text_path, texts, results, frame_info, output_path):
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        report_path = output_path / "description.txt"

        print(f"ğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š: {report_path}")

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("CLIPè§†é¢‘åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            # åŸºæœ¬ä¿¡æ¯
            f.write("ğŸ“ æ–‡ä»¶ä¿¡æ¯:\n")
            f.write(f"   è§†é¢‘æ–‡ä»¶: {Path(video_path).name}\n")
            f.write(f"   æ–‡æœ¬æ–‡ä»¶: {Path(text_path).name}\n")
            f.write(f"   åˆ†æå¸§æ•°: {len(results)}\n")
            f.write(f"   ä½¿ç”¨è®¾å¤‡: {device}\n")
            f.write(f"   æ¨¡å‹çŠ¶æ€: {'çœŸå®CLIPæ¨¡å‹' if self.model_loaded else 'æ¼”ç¤ºæ¨¡å¼'}\n\n")

            # æ–‡æœ¬æŸ¥è¯¢
            f.write("ğŸ“ æ–‡æœ¬æŸ¥è¯¢:\n")
            for i, text in enumerate(texts, 1):
                f.write(f"   {i}. {text}\n")
            f.write("\n")

            # æ•´ä½“åˆ†æç»“æœ
            f.write("ğŸ“Š æ•´ä½“åˆ†æç»“æœ:\n")
            f.write("-" * 30 + "\n")
            avg_scores = results.mean(dim=0)
            sorted_indices = torch.argsort(avg_scores, descending=True)

            for rank, idx in enumerate(sorted_indices, 1):
                text = texts[idx]
                score = avg_scores[idx].item()
                f.write(f"   {rank}. {text:30s}: {score:.4f}\n")
            f.write("\n")

            # é€å¸§è¯¦ç»†ç»“æœ
            f.write("ğŸï¸ é€å¸§åˆ†æç»“æœ:\n")
            f.write("-" * 50 + "\n")

            for i, (frame_probs, info) in enumerate(zip(results, frame_info)):
                f.write(f"\nå¸§ {i+1:3d} (æ—¶é—´: {info['timestamp']:6.2f}s, åŸå§‹å¸§å·: {info['frame_number']}):\n")

                # æŒ‰åˆ†æ•°æ’åº
                sorted_indices = torch.argsort(frame_probs, descending=True)
                for rank, idx in enumerate(sorted_indices, 1):
                    text = texts[idx]
                    score = frame_probs[idx].item()
                    f.write(f"   {rank}. {text:25s}: {score:.4f}\n")

            # æ—¶é—´æ®µåˆ†æ
            f.write(f"\nâ° æ—¶é—´æ®µåˆ†æ:\n")
            f.write("-" * 30 + "\n")

            # å°†è§†é¢‘åˆ†æˆå‡ ä¸ªæ—¶é—´æ®µ
            num_segments = min(5, len(results))
            segment_size = len(results) // num_segments

            for seg in range(num_segments):
                start_idx = seg * segment_size
                end_idx = min((seg + 1) * segment_size, len(results))

                if start_idx >= len(results):
                    break

                segment_results = results[start_idx:end_idx]
                segment_avg = segment_results.mean(dim=0)
                best_idx = torch.argmax(segment_avg)
                best_text = texts[best_idx]
                best_score = segment_avg[best_idx].item()

                start_time = frame_info[start_idx]['timestamp']
                end_time = frame_info[end_idx-1]['timestamp'] if end_idx > 0 else start_time

                f.write(f"   æ—¶é—´æ®µ {seg+1} ({start_time:.1f}s - {end_time:.1f}s): {best_text} ({best_score:.4f})\n")

    def _create_analysis_video(self, frames, texts, results, frame_info, output_path, fps=2):
        """åˆ›å»ºå¸¦æœ‰åˆ†æç»“æœçš„è§†é¢‘"""
        print(f"ğŸ¥ åˆ›å»ºåˆ†æè§†é¢‘: {output_path}")

        analysis_frames = []

        for i, (frame, frame_probs, info) in enumerate(zip(frames, results, frame_info)):
            # æ”¾å¤§å¸§åˆ°640x480ç”¨äºæ˜¾ç¤º
            display_frame = frame.resize((800, 600))
            from PIL import ImageDraw
            draw = ImageDraw.Draw(display_frame)

            # æ·»åŠ å¸§ä¿¡æ¯
            draw.text((10, 10), f"å¸§ {i+1}/{len(frames)}", fill=(255, 255, 255))
            draw.text((10, 35), f"æ—¶é—´: {info['timestamp']:.2f}s", fill=(255, 255, 255))

            # æ·»åŠ åˆ†æç»“æœ
            y_offset = 70
            sorted_indices = torch.argsort(frame_probs, descending=True)

            for rank, idx in enumerate(sorted_indices[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ªç»“æœ
                text = texts[idx]
                score = frame_probs[idx].item()
                color = (255, 255, 255) if score > 0.2 else (150, 150, 150)

                # ç»˜åˆ¶æ–‡æœ¬å’Œåˆ†æ•°
                result_text = f"{rank+1}. {text}: {score:.3f}"
                draw.text((10, y_offset + rank * 25), result_text, fill=color)

                # ç»˜åˆ¶åˆ†æ•°æ¡
                bar_width = int(score * 300)
                if bar_width > 0:
                    draw.rectangle([
                        400, y_offset + rank * 25 + 5,
                        400 + bar_width, y_offset + rank * 25 + 15
                    ], fill=(0, 255, 0) if score > 0.2 else (100, 100, 100))

            # è½¬æ¢ä¸ºOpenCVæ ¼å¼
            frame_array = np.array(display_frame)
            frame_bgr = cv2.cvtColor(frame_array, cv2.COLOR_RGB2BGR)
            analysis_frames.append(frame_bgr)

        # åˆ›å»ºè§†é¢‘
        if analysis_frames:
            height, width, layers = analysis_frames[0].shape
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            video_writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))

            for frame in analysis_frames:
                video_writer.write(frame)

            video_writer.release()
            print(f"âœ… åˆ†æè§†é¢‘å·²ä¿å­˜")

    def _save_key_frames(self, frames, texts, results, frame_info, output_path):
        """ä¿å­˜å…³é”®å¸§"""
        key_frames_dir = output_path / "key_frames"
        key_frames_dir.mkdir(exist_ok=True)

        print(f"ğŸ–¼ï¸ ä¿å­˜å…³é”®å¸§åˆ°: {key_frames_dir}")

        # ä¸ºæ¯ä¸ªæ–‡æœ¬æŸ¥è¯¢æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„å¸§
        for text_idx, text in enumerate(texts):
            text_scores = results[:, text_idx]
            best_frame_idx = torch.argmax(text_scores)
            best_score = text_scores[best_frame_idx].item()

            if best_score > 0.1:  # åªä¿å­˜åˆ†æ•°è¾ƒé«˜çš„å¸§
                best_frame = frames[best_frame_idx]
                best_info = frame_info[best_frame_idx]

                # æ¸…ç†æ–‡ä»¶å
                safe_text = "".join(c for c in text if c.isalnum() or c in (' ', '-', '_')).rstrip()
                safe_text = safe_text.replace(' ', '_')[:50]  # é™åˆ¶é•¿åº¦

                frame_filename = f"{text_idx+1:02d}_{safe_text}_{best_score:.3f}.jpg"
                frame_path = key_frames_dir / frame_filename

                best_frame.save(frame_path)
                print(f"   ä¿å­˜: {frame_filename} (æ—¶é—´: {best_info['timestamp']:.2f}s)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ CLIPè§†é¢‘åˆ†æå™¨ v2.0")
    print("=" * 80)

    # è®¾ç½®è¾“å…¥è·¯å¾„
    video_dir = Path("./videos/test/test1")
    video_path = video_dir / "test1.mp4"
    text_path = video_dir / "test1.txt"

    print(f"ğŸ“‚ è¾“å…¥æ–‡ä»¶:")
    print(f"   è§†é¢‘: {video_path}")
    print(f"   æ–‡æœ¬: {text_path}")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not video_path.exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        print("ğŸ’¡ è¯·å°†è§†é¢‘æ–‡ä»¶æ”¾åœ¨ ./videos/test/test1/test1.mp4")
        return

    if not text_path.exists():
        print(f"âŒ æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {text_path}")
        print("ğŸ’¡ è¯·åˆ›å»ºæ–‡æœ¬æ–‡ä»¶ ./videos/test/test1/test1.txt")
        print("   æ–‡ä»¶æ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢æ–‡æœ¬ï¼Œä¾‹å¦‚ï¼š")
        print("   a person walking")
        print("   a car driving")
        print("   a dog running")
        return

    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = VideoAnalyzer()

        # åˆ†æè§†é¢‘
        success = analyzer.analyze_video(
            video_path=str(video_path),
            text_path=str(text_path),
            output_dir=str(video_dir)
        )

        if success:
            print("\n" + "=" * 80)
            print("âœ… è§†é¢‘åˆ†æå®Œæˆï¼")
            print(f"ğŸ“ ç»“æœæ–‡ä»¶:")
            print(f"   ğŸ“Š è¯¦ç»†æŠ¥å‘Š: {video_dir}/description.txt")
            print(f"   ğŸ¥ åˆ†æè§†é¢‘: {video_dir}/analysis_video.mp4")
            print(f"   ğŸ–¼ï¸ å…³é”®å¸§: {video_dir}/key_frames/")
            print("=" * 80)
        else:
            print("âŒ è§†é¢‘åˆ†æå¤±è´¥")

    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

