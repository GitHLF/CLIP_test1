import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°"""

    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads

        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.out_linear = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None, debug=False):
        batch_size, seq_len, d_model = x.shape

        if debug:
            print(f"    ğŸ” MultiHeadAttention è¯¦ç»†è®¡ç®—è¿‡ç¨‹:")
            print(f"       è¾“å…¥ x shape: {x.shape}")
            print(f"       è¾“å…¥ x å‰3ä¸ªtokençš„å‰5ç»´æ•°å€¼:")
            for i in range(min(3, seq_len)):
                print(f"         token {i}: {x[0, i, :5].detach().numpy()}")

        # è®¡ç®—Q, K, V
        Q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        if debug:
            print(f"       Q shape: {Q.shape} (batch, heads, seq_len, head_dim)")
            print(f"       K shape: {K.shape}")
            print(f"       V shape: {V.shape}")
            print(f"       ç¬¬1ä¸ªå¤´çš„QçŸ©é˜µå‰3ä¸ªtokençš„å‰3ç»´:")
            for i in range(min(3, seq_len)):
                print(f"         Q[0,0,{i},:3] = {Q[0, 0, i, :3].detach().numpy()}")

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)

        if debug:
            print(f"       æ³¨æ„åŠ›åˆ†æ•° scores shape: {scores.shape}")
            print(f"       ç¼©æ”¾å› å­ âˆšd_k = âˆš{self.head_dim} = {math.sqrt(self.head_dim):.3f}")
            print(f"       ç¬¬1ä¸ªå¤´çš„æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ (å‰3x3):")
            print(f"         {scores[0, 0, :3, :3].detach().numpy()}")

        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(1)
            mask = mask.expand(batch_size, self.num_heads, seq_len, seq_len)
            scores = scores.masked_fill(mask == 0, -1e9)
            if debug:
                print(f"       åº”ç”¨maskåçš„åˆ†æ•° (å‰3x3):")
                print(f"         {scores[0, 0, :3, :3].detach().numpy()}")

        attention_weights = F.softmax(scores, dim=-1)

        if debug:
            print(f"       æ³¨æ„åŠ›æƒé‡ (softmaxå, å‰3x3):")
            print(f"         {attention_weights[0, 0, :3, :3].detach().numpy()}")
            print(f"       ç¬¬1è¡Œæƒé‡å’Œ: {attention_weights[0, 0, 0, :].sum().item():.6f}")

        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        context = torch.matmul(attention_weights, V)

        if debug:
            print(f"       ä¸Šä¸‹æ–‡å‘é‡ context shape: {context.shape}")
            print(f"       ç¬¬1ä¸ªå¤´ç¬¬1ä¸ªtokençš„ä¸Šä¸‹æ–‡å‘é‡å‰5ç»´:")
            print(f"         {context[0, 0, 0, :5].detach().numpy()}")

        # é‡æ–°ç»„ç»‡è¾“å‡º
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)

        output = self.out_linear(context)

        if debug:
            print(f"       æœ€ç»ˆè¾“å‡º shape: {output.shape}")
            print(f"       è¾“å‡ºç¬¬1ä¸ªtokenå‰5ç»´: {output[0, 0, :5].detach().numpy()}")

        return output


class TransformerBlock(nn.Module):
    """Transformerç¼–ç å™¨å—"""

    def __init__(self, d_model, num_heads, mlp_dim, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.mlp = nn.Sequential(
            nn.Linear(d_model, mlp_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(mlp_dim, d_model),
            nn.Dropout(dropout)
        )

    def forward(self, x, mask=None, debug=False):
        if debug:
            print(f"  ğŸ” TransformerBlock è¯¦ç»†è®¡ç®—è¿‡ç¨‹:")
            print(f"     è¾“å…¥ x shape: {x.shape}")
            print(f"     è¾“å…¥ç¬¬1ä¸ªtokenå‰5ç»´: {x[0, 0, :5].detach().numpy()}")

        # å¤šå¤´æ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        attn_output = self.attention(x, mask, debug=debug)

        if debug:
            print(f"     æ³¨æ„åŠ›è¾“å‡ºç¬¬1ä¸ªtokenå‰5ç»´: {attn_output[0, 0, :5].detach().numpy()}")

        x_after_attn = x + attn_output

        if debug:
            print(f"     æ®‹å·®è¿æ¥åç¬¬1ä¸ªtokenå‰5ç»´: {x_after_attn[0, 0, :5].detach().numpy()}")

        x_norm1 = self.norm1(x_after_attn)

        if debug:
            print(f"     LayerNorm1åç¬¬1ä¸ªtokenå‰5ç»´: {x_norm1[0, 0, :5].detach().numpy()}")

        # MLP + æ®‹å·®è¿æ¥
        mlp_output = self.mlp(x_norm1)

        if debug:
            print(f"     MLPè¾“å‡ºç¬¬1ä¸ªtokenå‰5ç»´: {mlp_output[0, 0, :5].detach().numpy()}")

        x_after_mlp = x_norm1 + mlp_output

        if debug:
            print(f"     MLPæ®‹å·®è¿æ¥åç¬¬1ä¸ªtokenå‰5ç»´: {x_after_mlp[0, 0, :5].detach().numpy()}")

        x_final = self.norm2(x_after_mlp)

        if debug:
            print(f"     LayerNorm2å(æœ€ç»ˆè¾“å‡º)ç¬¬1ä¸ªtokenå‰5ç»´: {x_final[0, 0, :5].detach().numpy()}")

        return x_final


class VisionTransformer(nn.Module):
    """Vision Transformer å›¾åƒç¼–ç å™¨"""

    def __init__(self, image_size=224, patch_size=16, num_channels=3,
                 d_model=768, num_heads=12, num_layers=12, mlp_dim=3072):
        super().__init__()
        self.image_size = image_size
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2
        self.d_model = d_model

        # Patch embeddingå±‚
        self.patch_embedding = nn.Conv2d(
            num_channels, d_model, kernel_size=patch_size, stride=patch_size
        )

        # ä½ç½®ç¼–ç 
        self.position_embedding = nn.Parameter(
            torch.randn(1, self.num_patches + 1, d_model)
        )

        # CLS token
        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))

        # Transformerç¼–ç å™¨å±‚
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, mlp_dim)
            for _ in range(num_layers)
        ])

        self.norm = nn.LayerNorm(d_model)

    def forward(self, x, debug=False):
        batch_size = x.shape[0]

        if debug:
            print(f"ğŸ” VisionTransformer è¯¦ç»†è®¡ç®—è¿‡ç¨‹:")
            print(f"   è¾“å…¥å›¾åƒ shape: {x.shape}")
            print(f"   å›¾åƒåƒç´ å€¼èŒƒå›´: [{x.min().item():.3f}, {x.max().item():.3f}]")
            print(f"   å·¦ä¸Šè§’3x3åƒç´ å€¼ (ç¬¬1ä¸ªé€šé“):")
            print(f"     {x[0, 0, :3, :3].detach().numpy()}")

        # å°†å›¾åƒåˆ†å‰²æˆpatcheså¹¶åµŒå…¥
        patches = self.patch_embedding(x)  # (B, d_model, H/P, W/P)

        if debug:
            print(f"   Patch embeddingå shape: {patches.shape}")
            print(f"   ç¬¬1ä¸ªpatchçš„å‰5ç»´ç‰¹å¾: {patches[0, :5, 0, 0].detach().numpy()}")

        x = patches.flatten(2).transpose(1, 2)  # (B, num_patches, d_model)

        if debug:
            print(f"   å±•å¹³å shape: {x.shape}")
            print(f"   ç¬¬1ä¸ªpatchå‰5ç»´: {x[0, 0, :5].detach().numpy()}")

        # æ·»åŠ CLS token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)

        if debug:
            print(f"   CLS token shape: {cls_tokens.shape}")
            print(f"   CLS tokenå‰5ç»´: {cls_tokens[0, 0, :5].detach().numpy()}")

        x = torch.cat([cls_tokens, x], dim=1)

        if debug:
            print(f"   æ·»åŠ CLS tokenå shape: {x.shape}")

        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.position_embedding

        if debug:
            print(f"   ä½ç½®ç¼–ç  shape: {self.position_embedding.shape}")
            print(f"   ä½ç½®ç¼–ç å‰5ç»´ (CLS): {self.position_embedding[0, 0, :5].detach().numpy()}")
            print(f"   ä½ç½®ç¼–ç å‰5ç»´ (ç¬¬1ä¸ªpatch): {self.position_embedding[0, 1, :5].detach().numpy()}")
            print(f"   æ·»åŠ ä½ç½®ç¼–ç åCLS tokenå‰5ç»´: {x[0, 0, :5].detach().numpy()}")

        # é€šè¿‡Transformerå±‚
        for i, block in enumerate(self.transformer_blocks):
            if debug and i < 2:  # åªæ˜¾ç¤ºå‰2å±‚çš„è¯¦ç»†ä¿¡æ¯
                print(f"\n   === Transformer Block {i+1} ===")
                x = block(x, debug=True)
            else:
                x = block(x, debug=False)

        x = self.norm(x)

        if debug:
            print(f"   æœ€ç»ˆLayerNormå shape: {x.shape}")
            print(f"   CLS tokenæœ€ç»ˆç‰¹å¾å‰5ç»´: {x[0, 0, :5].detach().numpy()}")

        # è¿”å›CLS tokençš„ç‰¹å¾
        return x[:, 0]


class TextTransformer(nn.Module):
    """æ–‡æœ¬Transformerç¼–ç å™¨"""

    def __init__(self, vocab_size=49408, max_length=77, d_model=512,
                 num_heads=8, num_layers=12, mlp_dim=2048):
        super().__init__()
        self.d_model = d_model
        self.max_length = max_length

        # Token embedding
        self.token_embedding = nn.Embedding(vocab_size, d_model)

        # ä½ç½®ç¼–ç 
        self.position_embedding = nn.Parameter(
            torch.randn(1, max_length, d_model)
        )

        # Transformerç¼–ç å™¨å±‚
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(d_model, num_heads, mlp_dim)
            for _ in range(num_layers)
        ])

        self.norm = nn.LayerNorm(d_model)

    def forward(self, input_ids, attention_mask=None, debug=False):
        batch_size, seq_len = input_ids.shape

        if debug:
            print(f"ğŸ” TextTransformer è¯¦ç»†è®¡ç®—è¿‡ç¨‹:")
            print(f"   è¾“å…¥token IDs shape: {input_ids.shape}")
            print(f"   å‰10ä¸ªtoken IDs: {input_ids[0, :10].detach().numpy()}")
            if attention_mask is not None:
                print(f"   attention_maskå‰10ä½: {attention_mask[0, :10].detach().numpy()}")

        # Token embedding + ä½ç½®ç¼–ç 
        token_embeds = self.token_embedding(input_ids)

        if debug:
            print(f"   Token embedding shape: {token_embeds.shape}")
            print(f"   ç¬¬1ä¸ªtoken embeddingå‰5ç»´: {token_embeds[0, 0, :5].detach().numpy()}")
            print(f"   ç¬¬2ä¸ªtoken embeddingå‰5ç»´: {token_embeds[0, 1, :5].detach().numpy()}")

        pos_embed = self.position_embedding[:, :seq_len, :]

        if debug:
            print(f"   ä½ç½®ç¼–ç  shape: {pos_embed.shape}")
            print(f"   ä½ç½®0ç¼–ç å‰5ç»´: {pos_embed[0, 0, :5].detach().numpy()}")
            print(f"   ä½ç½®1ç¼–ç å‰5ç»´: {pos_embed[0, 1, :5].detach().numpy()}")

        x = token_embeds + pos_embed

        if debug:
            print(f"   Token + ä½ç½®ç¼–ç åç¬¬1ä¸ªtokenå‰5ç»´: {x[0, 0, :5].detach().numpy()}")

        # é€šè¿‡Transformerå±‚
        for i, block in enumerate(self.transformer_blocks):
            if debug and i < 2:  # åªæ˜¾ç¤ºå‰2å±‚çš„è¯¦ç»†ä¿¡æ¯
                print(f"\n   === Text Transformer Block {i+1} ===")
                x = block(x, attention_mask, debug=True)
            else:
                x = block(x, attention_mask, debug=False)

        x = self.norm(x)

        # è¿”å›æœ€åä¸€ä¸ªæœ‰æ•ˆtokençš„ç‰¹å¾
        if attention_mask is not None:
            last_token_indices = (attention_mask.sum(dim=1) - 1).long()
            batch_indices = torch.arange(x.shape[0], device=x.device)
            result = x[batch_indices, last_token_indices]

            if debug:
                print(f"   æœ€åæœ‰æ•ˆtokenä½ç½®: {last_token_indices.detach().numpy()}")
                print(f"   æœ€åæœ‰æ•ˆtokenç‰¹å¾å‰5ç»´: {result[0, :5].detach().numpy()}")
        else:
            result = x[:, -1]
            if debug:
                print(f"   ä½¿ç”¨æœ€åä¸€ä¸ªtokenç‰¹å¾å‰5ç»´: {result[0, :5].detach().numpy()}")

        return result


class SimpleCLIP(nn.Module):
    """ç®€åŒ–ç‰ˆCLIPæ¨¡å‹å®ç°"""

    def __init__(self, image_encoder_config=None, text_encoder_config=None, projection_dim=512):
        super().__init__()

        # é»˜è®¤é…ç½®
        if image_encoder_config is None:
            image_encoder_config = {
                'image_size': 224, 'patch_size': 16, 'num_channels': 3,
                'd_model': 768, 'num_heads': 12, 'num_layers': 12, 'mlp_dim': 3072
            }

        if text_encoder_config is None:
            text_encoder_config = {
                'vocab_size': 49408, 'max_length': 77, 'd_model': 512,
                'num_heads': 8, 'num_layers': 12, 'mlp_dim': 2048
            }

        # å›¾åƒç¼–ç å™¨
        self.vision_model = VisionTransformer(**image_encoder_config)

        # æ–‡æœ¬ç¼–ç å™¨
        self.text_model = TextTransformer(**text_encoder_config)

        # æŠ•å½±å±‚
        self.visual_projection = nn.Linear(image_encoder_config['d_model'], projection_dim)
        self.text_projection = nn.Linear(text_encoder_config['d_model'], projection_dim)

        # æ¸©åº¦å‚æ•°
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

    def encode_image(self, image, debug=False):
        """ç¼–ç å›¾åƒ"""
        if debug:
            print(f"ğŸ” å›¾åƒç¼–ç è¿‡ç¨‹:")

        image_features = self.vision_model(image, debug=debug)

        if debug:
            print(f"   Visionæ¨¡å‹è¾“å‡º shape: {image_features.shape}")
            print(f"   Visionç‰¹å¾å‰5ç»´: {image_features[0, :5].detach().numpy()}")

        projected_features = self.visual_projection(image_features)

        if debug:
            print(f"   æŠ•å½±åç‰¹å¾ shape: {projected_features.shape}")
            print(f"   æŠ•å½±åç‰¹å¾å‰5ç»´: {projected_features[0, :5].detach().numpy()}")

        normalized_features = F.normalize(projected_features, dim=-1)

        if debug:
            print(f"   å½’ä¸€åŒ–åç‰¹å¾å‰5ç»´: {normalized_features[0, :5].detach().numpy()}")
            print(f"   ç‰¹å¾å‘é‡é•¿åº¦: {torch.norm(normalized_features[0]).item():.6f}")

        return normalized_features

    def encode_text(self, input_ids, attention_mask=None, debug=False):
        """ç¼–ç æ–‡æœ¬"""
        if debug:
            print(f"ğŸ” æ–‡æœ¬ç¼–ç è¿‡ç¨‹:")

        text_features = self.text_model(input_ids, attention_mask, debug=debug)

        if debug:
            print(f"   Textæ¨¡å‹è¾“å‡º shape: {text_features.shape}")
            print(f"   Textç‰¹å¾å‰5ç»´: {text_features[0, :5].detach().numpy()}")

        projected_features = self.text_projection(text_features)

        if debug:
            print(f"   æŠ•å½±åç‰¹å¾ shape: {projected_features.shape}")
            print(f"   æŠ•å½±åç‰¹å¾å‰5ç»´: {projected_features[0, :5].detach().numpy()}")

        normalized_features = F.normalize(projected_features, dim=-1)

        if debug:
            print(f"   å½’ä¸€åŒ–åç‰¹å¾å‰5ç»´: {normalized_features[0, :5].detach().numpy()}")
            print(f"   ç‰¹å¾å‘é‡é•¿åº¦: {torch.norm(normalized_features[0]).item():.6f}")

        return normalized_features

    def forward(self, image, input_ids, attention_mask=None, debug=False):
        """å‰å‘ä¼ æ’­"""
        if debug:
            print(f"ğŸš€ CLIPå®Œæ•´å‰å‘ä¼ æ’­è¿‡ç¨‹:")
            print(f"=" * 80)

        image_features = self.encode_image(image, debug=debug)

        if debug:
            print(f"\n" + "=" * 80)

        text_features = self.encode_text(input_ids, attention_mask, debug=debug)

        if debug:
            print(f"\nğŸ” ç›¸ä¼¼åº¦è®¡ç®—è¿‡ç¨‹:")
            print(f"   å›¾åƒç‰¹å¾ shape: {image_features.shape}")
            print(f"   æ–‡æœ¬ç‰¹å¾ shape: {text_features.shape}")

        # è®¡ç®—ç›¸ä¼¼åº¦
        logit_scale = self.logit_scale.exp()

        if debug:
            print(f"   æ¸©åº¦å‚æ•° logit_scale: {logit_scale.item():.6f}")

        # è®¡ç®—ç‚¹ç§¯
        dot_product = image_features @ text_features.t()

        if debug:
            print(f"   ç‚¹ç§¯ç»“æœ shape: {dot_product.shape}")
            print(f"   ç‚¹ç§¯çŸ©é˜µ:")
            print(f"     {dot_product.detach().numpy()}")

        logits_per_image = logit_scale * dot_product
        logits_per_text = logits_per_image.t()

        if debug:
            print(f"   ç¼©æ”¾åçš„logits_per_image:")
            print(f"     {logits_per_image.detach().numpy()}")
            print(f"   logits_per_text:")
            print(f"     {logits_per_text.detach().numpy()}")

            # è®¡ç®—æ¦‚ç‡
            probs_i2t = F.softmax(logits_per_image, dim=-1)
            probs_t2i = F.softmax(logits_per_text, dim=-1)

            print(f"   å›¾åƒåˆ°æ–‡æœ¬æ¦‚ç‡:")
            print(f"     {probs_i2t.detach().numpy()}")
            print(f"   æ–‡æœ¬åˆ°å›¾åƒæ¦‚ç‡:")
            print(f"     {probs_t2i.detach().numpy()}")

        return {
            'logits_per_image': logits_per_image,
            'logits_per_text': logits_per_text,
            'image_embeds': image_features,
            'text_embeds': text_features
        }


class CLIPExplanation:
    """
    CLIP (Contrastive Language-Image Pre-training) æ¨¡å‹è¯¦ç»†è®²è§£

    è¿™æ˜¯ä¸€ä¸ªä»é›¶å®ç°çš„CLIPæ¨¡å‹ï¼Œä¸ä¾èµ–transformersåº“
    """

    def __init__(self):
        # åˆ›å»ºç®€åŒ–ç‰ˆCLIPæ¨¡å‹
        self.model = SimpleCLIP()
        print("âœ… æˆåŠŸåˆ›å»ºç®€åŒ–ç‰ˆCLIPæ¨¡å‹ï¼ˆæ— éœ€transformersåº“ï¼‰")

    def detailed_computation_example(self):
        """è¯¦ç»†çš„è®¡ç®—è¿‡ç¨‹ç¤ºä¾‹"""
        print("=" * 80)
        print("CLIP è¯¦ç»†è®¡ç®—è¿‡ç¨‹ç¤ºä¾‹")
        print("=" * 80)

        # åˆ›å»ºå…·ä½“çš„ç¤ºä¾‹æ•°æ®
        print("\nğŸ“ åˆ›å»ºç¤ºä¾‹æ•°æ®:")

        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å›¾åƒï¼ˆçº¢è‰²æ–¹å—ï¼‰
        image = torch.zeros(1, 3, 224, 224)
        image[0, 0, 50:150, 50:150] = 1.0  # çº¢è‰²é€šé“
        print(f"   åˆ›å»ºäº†ä¸€ä¸ªçº¢è‰²æ–¹å—å›¾åƒ shape: {image.shape}")

        # åˆ›å»ºæ–‡æœ¬tokensï¼ˆæ¨¡æ‹Ÿ "a red square" çš„tokenizationï¼‰
        input_ids = torch.tensor([[49406, 320, 1000, 5000, 49407] + [0] * 72])  # æ¨¡æ‹Ÿtokenåºåˆ—
        attention_mask = torch.tensor([[1, 1, 1, 1, 1] + [0] * 72])  # å‰5ä¸ªtokenæœ‰æ•ˆ

        print(f"   æ–‡æœ¬tokens: {input_ids[0, :10].numpy()} (å‰10ä¸ª)")
        print(f"   æ³¨æ„åŠ›mask: {attention_mask[0, :10].numpy()} (å‰10ä¸ª)")

        print(f"\nğŸš€ å¼€å§‹CLIPå®Œæ•´è®¡ç®—è¿‡ç¨‹:")
        print("=" * 80)

        # æ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œå¼€å¯debugæ¨¡å¼
        with torch.no_grad():
            outputs = self.model(image, input_ids, attention_mask, debug=True)

        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦: {outputs['logits_per_image'][0, 0].item():.6f}")
        print(f"   å›¾åƒç‰¹å¾å‘é‡é•¿åº¦: {torch.norm(outputs['image_embeds'][0]).item():.6f}")
        print(f"   æ–‡æœ¬ç‰¹å¾å‘é‡é•¿åº¦: {torch.norm(outputs['text_embeds'][0]).item():.6f}")

    def attention_visualization_example(self):
        """æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–ç¤ºä¾‹"""
        print("\n" + "=" * 80)
        print("æ³¨æ„åŠ›æœºåˆ¶è¯¦ç»†è®¡ç®—ç¤ºä¾‹")
        print("=" * 80)

        # åˆ›å»ºä¸€ä¸ªå°çš„ç¤ºä¾‹æ¥æ¼”ç¤ºæ³¨æ„åŠ›è®¡ç®—
        print("\nğŸ“ åˆ›å»ºç®€åŒ–çš„æ³¨æ„åŠ›ç¤ºä¾‹:")
        d_model = 8  # ç®€åŒ–çš„æ¨¡å‹ç»´åº¦
        num_heads = 2
        seq_len = 4
        batch_size = 1

        # åˆ›å»ºç®€å•çš„è¾“å…¥
        x = torch.randn(batch_size, seq_len, d_model)
        print(f"   è¾“å…¥åºåˆ— shape: {x.shape}")
        print(f"   è¾“å…¥çŸ©é˜µ:")
        for i in range(seq_len):
            print(f"     token {i}: {x[0, i].detach().numpy()}")

        # åˆ›å»ºæ³¨æ„åŠ›å±‚
        attention = MultiHeadAttention(d_model, num_heads)

        print(f"\nğŸ” æ³¨æ„åŠ›è®¡ç®—è¯¦ç»†è¿‡ç¨‹:")
        with torch.no_grad():
            output = attention(x, debug=True)

        print(f"\nğŸ“Š æ³¨æ„åŠ›è®¡ç®—æ€»ç»“:")
        print(f"   è¾“å…¥shape: {x.shape}")
        print(f"   è¾“å‡ºshape: {output.shape}")
        print(f"   è¾“å‡ºçŸ©é˜µ:")
        for i in range(seq_len):
            print(f"     token {i}: {output[0, i].detach().numpy()}")

    def step_by_step_training_example(self):
        """é€æ­¥è®­ç»ƒè¿‡ç¨‹ç¤ºä¾‹"""
        print("\n" + "=" * 80)
        print("CLIPè®­ç»ƒè¿‡ç¨‹è¯¦ç»†ç¤ºä¾‹")
        print("=" * 80)

        print("\nğŸ“ æ¨¡æ‹Ÿè®­ç»ƒbatch:")
        batch_size = 3

        # åˆ›å»ºbatchæ•°æ®
        images = torch.randn(batch_size, 3, 224, 224)
        input_ids = torch.randint(1, 1000, (batch_size, 77))
        attention_mask = torch.ones(batch_size, 77)

        print(f"   Batchå¤§å°: {batch_size}")
        print(f"   å›¾åƒbatch shape: {images.shape}")
        print(f"   æ–‡æœ¬batch shape: {input_ids.shape}")

        print(f"\nğŸ” è®­ç»ƒå‰å‘ä¼ æ’­:")
        with torch.no_grad():
            # ç¼–ç å›¾åƒå’Œæ–‡æœ¬
            image_features = self.model.encode_image(images)
            text_features = self.model.encode_text(input_ids, attention_mask)

            print(f"   å›¾åƒç‰¹å¾ shape: {image_features.shape}")
            print(f"   æ–‡æœ¬ç‰¹å¾ shape: {text_features.shape}")

            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            logit_scale = self.model.logit_scale.exp()
            similarity_matrix = logit_scale * image_features @ text_features.t()

            print(f"\nğŸ“Š ç›¸ä¼¼åº¦çŸ©é˜µ ({batch_size}x{batch_size}):")
            print(f"   {similarity_matrix.detach().numpy()}")

            # åˆ›å»ºæ ‡ç­¾ï¼ˆå¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬ï¼‰
            labels = torch.arange(batch_size)
            print(f"   æ ‡ç­¾: {labels.numpy()}")

            # è®¡ç®—æŸå¤±
            loss_i2t = F.cross_entropy(similarity_matrix, labels)
            loss_t2i = F.cross_entropy(similarity_matrix.t(), labels)
            total_loss = (loss_i2t + loss_t2i) / 2

            print(f"\nğŸ’° æŸå¤±è®¡ç®—:")
            print(f"   å›¾åƒåˆ°æ–‡æœ¬æŸå¤±: {loss_i2t.item():.6f}")
            print(f"   æ–‡æœ¬åˆ°å›¾åƒæŸå¤±: {loss_t2i.item():.6f}")
            print(f"   æ€»æŸå¤±: {total_loss.item():.6f}")

            # æ˜¾ç¤ºæ¦‚ç‡åˆ†å¸ƒ
            probs_i2t = F.softmax(similarity_matrix, dim=-1)
            probs_t2i = F.softmax(similarity_matrix.t(), dim=-1)

            print(f"\nğŸ“ˆ æ¦‚ç‡åˆ†å¸ƒ:")
            print(f"   å›¾åƒåˆ°æ–‡æœ¬æ¦‚ç‡:")
            for i in range(batch_size):
                print(f"     å›¾åƒ{i}: {probs_i2t[i].detach().numpy()}")
            print(f"   æ–‡æœ¬åˆ°å›¾åƒæ¦‚ç‡:")
            for i in range(batch_size):
                print(f"     æ–‡æœ¬{i}: {probs_t2i[i].detach().numpy()}")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡ŒCLIPè¯¦ç»†è®²è§£"""
    print("CLIP (Contrastive Language-Image Pre-training) è¯¦ç»†è®¡ç®—è¿‡ç¨‹è®²è§£")
    print("ğŸš€ ä»é›¶å®ç°ç‰ˆæœ¬ - åŒ…å«æ‰€æœ‰ä¸­é—´å€¼")
    print("=" * 80)

    try:
        # åˆ›å»ºè®²è§£å®ä¾‹
        clip_demo = CLIPExplanation()

        # 1. è¯¦ç»†è®¡ç®—è¿‡ç¨‹ç¤ºä¾‹
        clip_demo.detailed_computation_example()

        # 2. æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–
        clip_demo.attention_visualization_example()

        # 3. è®­ç»ƒè¿‡ç¨‹ç¤ºä¾‹
        clip_demo.step_by_step_training_example()

        print("\n" + "=" * 80)
        print("âœ… CLIP è¯¦ç»†è®¡ç®—è¿‡ç¨‹è®²è§£å®Œæˆï¼")
        print("ğŸ’¡ æ‰€æœ‰ä¸­é—´å€¼éƒ½å·²æ˜¾ç¤ºï¼Œå¸®åŠ©ç†è§£æ¯ä¸€æ­¥çš„è®¡ç®—è¿‡ç¨‹")
        print("=" * 80)

    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()