import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# è®¾ç½®HuggingFaceé•œåƒæº
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# å¿½ç•¥è­¦å‘Š
import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn.functional as F
from PIL import Image
import json
from pathlib import Path


# ç›´æ¥å¯¼å…¥æ ¸å¿ƒç»„ä»¶ï¼Œç»•è¿‡image_processingé—®é¢˜
def load_clip_directly():
    """ç›´æ¥åŠ è½½CLIPæ¨¡å‹ï¼Œç»•è¿‡image_processingé—®é¢˜"""
    try:
        # ç›´æ¥ä»modelingæ¨¡å—å¯¼å…¥ï¼Œé¿å…processingé—®é¢˜
        from transformers.models.clip.modeling_clip import CLIPModel
        from transformers.models.clip.configuration_clip import CLIPConfig
        from transformers import AutoTokenizer

        print("âœ… æˆåŠŸå¯¼å…¥æ ¸å¿ƒCLIPç»„ä»¶")
        return CLIPModel, CLIPConfig, AutoTokenizer, True
    except ImportError as e:
        print(f"âŒ æ ¸å¿ƒç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
        return None, None, None, False

# åˆ›å»ºç®€åŒ–çš„å¤„ç†å™¨
class SimpleProcessor:
    """ç®€åŒ–çš„CLIPå¤„ç†å™¨ï¼Œé¿å…image_processingé—®é¢˜"""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, text=None, images=None, return_tensors="pt", padding=True, debug=False):
        if debug:
            print("ğŸ” SimpleProcessor è¯¦ç»†å¤„ç†è¿‡ç¨‹:")

        # å¤„ç†æ–‡æœ¬
        if text is not None:
            if isinstance(text, str):
                text = [text]

            if debug:
                print(f"   è¾“å…¥æ–‡æœ¬: {text}")

            # ä½¿ç”¨tokenizerå¤„ç†æ–‡æœ¬
            text_inputs = self.tokenizer(
                text,
                return_tensors=return_tensors,
                padding=padding,
                truncation=True,
                max_length=77
            )

            if debug:
                print(f"   Tokenizationç»“æœ:")
                for i, txt in enumerate(text):
                    tokens = text_inputs['input_ids'][i]
                    print(f"     æ–‡æœ¬{i}: '{txt}'")
                    print(f"     Token IDs: {tokens[:10].tolist()}... (å‰10ä¸ª)")
                    print(f"     Tokené•¿åº¦: {tokens.shape[0]}")
                    print(f"     å®é™…æœ‰æ•ˆé•¿åº¦: {text_inputs['attention_mask'][i].sum().item()}")
        else:
            text_inputs = {}

        # å¤„ç†å›¾åƒ
        if images is not None:
            if isinstance(images, Image.Image):
                images = [images]

            if debug:
                print(f"   è¾“å…¥å›¾åƒæ•°é‡: {len(images)}")
                print(f"   ç¬¬1å¼ å›¾åƒå°ºå¯¸: {images[0].size}")

            # ç®€å•çš„å›¾åƒé¢„å¤„ç†
            pixel_values = []
            for i, img in enumerate(images):
                if debug and i == 0:  # åªæ˜¾ç¤ºç¬¬ä¸€å¼ å›¾åƒçš„è¯¦ç»†å¤„ç†
                    print(f"   å›¾åƒ{i}é¢„å¤„ç†è¿‡ç¨‹:")
                    print(f"     åŸå§‹å°ºå¯¸: {img.size}")

                # è°ƒæ•´å¤§å°åˆ°224x224
                img = img.resize((224, 224))
                if debug and i == 0:
                    print(f"     è°ƒæ•´åå°ºå¯¸: {img.size}")

                # è½¬æ¢ä¸ºtensor
                img_array = torch.tensor(list(img.getdata())).float()
                img_array = img_array.view(224, 224, 3)

                if debug and i == 0:
                    print(f"     è½¬æ¢ä¸ºtensor shape: {img_array.shape}")
                    print(f"     åƒç´ å€¼èŒƒå›´: [{img_array.min():.1f}, {img_array.max():.1f}]")
                    print(f"     å·¦ä¸Šè§’3x3åƒç´  (Ré€šé“):")
                    print(f"       {img_array[:3, :3, 0].numpy()}")

                # è½¬æ¢ä¸ºCHWæ ¼å¼å¹¶å½’ä¸€åŒ–
                img_tensor = img_array.permute(2, 0, 1) / 255.0

                if debug and i == 0:
                    print(f"     CHWæ ¼å¼ shape: {img_tensor.shape}")
                    print(f"     å½’ä¸€åŒ–åèŒƒå›´: [{img_tensor.min():.3f}, {img_tensor.max():.3f}]")

                # æ ‡å‡†åŒ– (ImageNetæ ‡å‡†)
                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                img_tensor = (img_tensor - mean) / std

                if debug and i == 0:
                    print(f"     ImageNetæ ‡å‡†åŒ–å:")
                    print(f"       å‡å€¼: {mean.flatten().tolist()}")
                    print(f"       æ ‡å‡†å·®: {std.flatten().tolist()}")
                    print(f"       æ ‡å‡†åŒ–åèŒƒå›´: [{img_tensor.min():.3f}, {img_tensor.max():.3f}]")
                    print(f"       Ré€šé“å·¦ä¸Šè§’3x3:")
                    print(f"         {img_tensor[0, :3, :3].numpy()}")

                pixel_values.append(img_tensor)

            pixel_values = torch.stack(pixel_values)
            text_inputs['pixel_values'] = pixel_values

            if debug:
                print(f"   æœ€ç»ˆpixel_values shape: {pixel_values.shape}")

        return text_inputs

# å°è¯•å¯¼å…¥ç»„ä»¶
CLIPModel, CLIPConfig, AutoTokenizer, clip_available = load_clip_directly()

class CLIPExplanation:
    """
    CLIP (Contrastive Language-Image Pre-training) æ¨¡å‹è¯¦ç»†è®²è§£

    CLIPæ˜¯OpenAIå¼€å‘çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å…³ç³»
    é€šè¿‡å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼ï¼Œå°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€ä¸ªç‰¹å¾ç©ºé—´ä¸­
    """

    def __init__(self, use_mirror=True, mirror_url='https://hf-mirror.com'):
        print("ğŸ”„ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒçš„CLIPæ¨¡å‹...")

        if not clip_available:
            print("âŒ CLIPç»„ä»¶ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
            self._create_demo_version()
            return

        model_loaded = False

        # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶
        model_dir = Path("./models")
        possible_paths = [
            model_dir / "models--openai--clip-vit-base-patch32" / "snapshots",
            model_dir / "clip-model",
            model_dir
        ]

        local_model_path = None
        for path in possible_paths:
            if path.exists():
                # æŸ¥æ‰¾åŒ…å«config.jsonçš„å­ç›®å½•
                for subdir in path.rglob("*"):
                    if subdir.is_dir() and (subdir / "config.json").exists():
                        local_model_path = subdir
                        break
                if local_model_path:
                    break

        if local_model_path:
            print(f"ğŸ“ æ‰¾åˆ°æœ¬åœ°æ¨¡å‹æ–‡ä»¶: {local_model_path}")

            try:
                print("ğŸ”§ ç›´æ¥ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ¨¡å‹...")

                # åŠ è½½é…ç½®
                config_path = local_model_path / "config.json"
                with open(config_path, 'r') as f:
                    config_dict = json.load(f)
                config = CLIPConfig.from_dict(config_dict)

                # åŠ è½½æ¨¡å‹
                self.model = CLIPModel.from_pretrained(
                    str(local_model_path),
                    config=config,
                    local_files_only=True
                )

                # åŠ è½½tokenizer
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        str(local_model_path),
                        local_files_only=True
                    )
                except:
                    # å¦‚æœæœ¬åœ°tokenizeråŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åœ¨çº¿ç‰ˆæœ¬
                    print("âš ï¸  æœ¬åœ°tokenizeråŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åœ¨çº¿ç‰ˆæœ¬...")
                    self.tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

                # åˆ›å»ºç®€åŒ–å¤„ç†å™¨
                self.processor = SimpleProcessor(self.tokenizer)

                print("âœ… æˆåŠŸä»æœ¬åœ°æ–‡ä»¶åŠ è½½CLIPæ¨¡å‹")
                model_loaded = True

            except Exception as e:
                print(f"âŒ æœ¬åœ°æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
                print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
                import traceback
                traceback.print_exc()

        # å¦‚æœæœ¬åœ°åŠ è½½å¤±è´¥ï¼Œå°è¯•åœ¨çº¿åŠ è½½
        if not model_loaded:
            try:
                print("ğŸŒ å°è¯•åœ¨çº¿åŠ è½½...")

                # ç›´æ¥ä½¿ç”¨æ¨¡å‹åç§°åŠ è½½
                self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
                self.tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")
                self.processor = SimpleProcessor(self.tokenizer)

                print("âœ… æˆåŠŸåœ¨çº¿åŠ è½½CLIPæ¨¡å‹")
                model_loaded = True

            except Exception as e:
                print(f"âŒ åœ¨çº¿åŠ è½½ä¹Ÿå¤±è´¥: {e}")

        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œåˆ›å»ºæ¼”ç¤ºç‰ˆæœ¬
        if not model_loaded:
            print("âŒ æ— æ³•åŠ è½½çœŸå®æ¨¡å‹ï¼Œåˆ›å»ºæ¼”ç¤ºç‰ˆæœ¬")
            self._create_demo_version()
            model_loaded = True

        if model_loaded and not hasattr(self, 'demo_mode'):
            print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
            print(f"   - æ¨¡å‹å¤§å°: ~605MB")
            print(f"   - ç¼“å­˜ä½ç½®: ./models/")
            print(f"   - å›¾åƒç¼–ç å™¨: Vision Transformer")
            print(f"   - æ–‡æœ¬ç¼–ç å™¨: Transformer")

            # æµ‹è¯•æ¨¡å‹
            self._test_model()

    def _test_model(self):
        """æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
        try:
            print("ğŸ§ª æµ‹è¯•æ¨¡å‹åŠŸèƒ½...")

            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_image = Image.new('RGB', (224, 224), color='red')
            test_texts = ["a red image", "a blue image"]

            # å¤„ç†è¾“å…¥
            inputs = self.processor(text=test_texts, images=test_image)

            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)

            print("âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸï¼")

        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")

    def _create_demo_version(self):
        """åˆ›å»ºä¸€ä¸ªç”¨äºæ¼”ç¤ºçš„ç®€åŒ–ç‰ˆæœ¬"""
        print("ğŸ”§ åˆ›å»ºæ¼”ç¤ºç‰ˆCLIPæ¨¡å‹...")

        class DemoModel:
            def __init__(self):
                self.config = type('Config', (), {
                    'hidden_size': 768,
                    'num_attention_heads': 12,
                    'image_size': 224,
                    'patch_size': 16,
                    'vocab_size': 49408,
                    'max_position_embeddings': 77
                })()

                self.vision_model = type('VisionModel', (), {
                    'config': self.config,
                    'encoder': type('Encoder', (), {
                        'layers': [None] * 12
                    })()
                })()

                self.text_model = type('TextModel', (), {
                    'config': self.config,
                    'encoder': type('Encoder', (), {
                        'layers': [None] * 12
                    })()
                })()

                self.visual_projection = type('Projection', (), {
                    'in_features': 768,
                    'out_features': 512
                })()

                self.text_projection = type('Projection', (), {
                    'in_features': 512,
                    'out_features': 512
                })()

                self.logit_scale = torch.tensor(2.6593)

            def parameters(self):
                return [torch.randn(1000) for _ in range(100)]

            def __call__(self, **kwargs):
                # æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡º
                batch_size = 1
                if 'pixel_values' in kwargs:
                    batch_size = kwargs['pixel_values'].shape[0]

                text_batch_size = 1
                if 'input_ids' in kwargs:
                    text_batch_size = kwargs['input_ids'].shape[0]

                return type('Output', (), {
                    'image_embeds': torch.randn(batch_size, 512),
                    'text_embeds': torch.randn(text_batch_size, 512),
                    'logits_per_image': torch.randn(batch_size, text_batch_size),
                    'logits_per_text': torch.randn(text_batch_size, batch_size)
                })()

        class DemoProcessor:
            def __call__(self, text=None, images=None, return_tensors="pt", padding=True, debug=False):
                batch_size = len(text) if text else 1
                return {
                    'pixel_values': torch.randn(1, 3, 224, 224),
                    'input_ids': torch.randint(0, 1000, (batch_size, 77)),
                    'attention_mask': torch.ones(batch_size, 77)
                }

        self.model = DemoModel()
        self.processor = DemoProcessor()
        self.demo_mode = True
        print("âœ… æ¼”ç¤ºç‰ˆCLIPæ¨¡å‹åˆ›å»ºå®Œæˆ")

    def explain_clip_architecture(self):
        """è¯¦ç»†è§£é‡ŠCLIPçš„å†…éƒ¨æ¶æ„"""
        print("=" * 60)
        print("CLIP æ¨¡å‹æ¶æ„è¯¦è§£")
        print("=" * 60)

        if hasattr(self, 'demo_mode'):
            print("âš ï¸  å½“å‰ä¸ºæ¼”ç¤ºæ¨¡å¼ï¼Œæ˜¾ç¤ºç†è®ºæ¶æ„")

        print("\n1. æ•´ä½“æ¶æ„:")
        print("   CLIP = å›¾åƒç¼–ç å™¨ + æ–‡æœ¬ç¼–ç å™¨ + å¯¹æ¯”å­¦ä¹ ")
        print("   - å›¾åƒç¼–ç å™¨: Vision Transformer (ViT) æˆ– ResNet")
        print("   - æ–‡æœ¬ç¼–ç å™¨: Transformer")
        print("   - ç›®æ ‡: å°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°åŒä¸€ä¸ªç‰¹å¾ç©ºé—´")

        print("\n2. å›¾åƒç¼–ç å™¨ (Vision Transformer):")
        print("   - å°†å›¾åƒåˆ†å‰²æˆå›ºå®šå¤§å°çš„patches (å¦‚16x16)")
        print("   - æ¯ä¸ªpatché€šè¿‡çº¿æ€§æŠ•å½±å˜æˆembedding")
        print("   - æ·»åŠ ä½ç½®ç¼–ç ")
        print("   - é€šè¿‡å¤šå±‚Transformerç¼–ç å™¨å¤„ç†")
        print("   - è¾“å‡º: å›¾åƒçš„ç‰¹å¾å‘é‡è¡¨ç¤º")

        print("\n3. æ–‡æœ¬ç¼–ç å™¨ (Transformer):")
        print("   - æ–‡æœ¬tokenizationå’Œembedding")
        print("   - æ·»åŠ ä½ç½®ç¼–ç ")
        print("   - é€šè¿‡å¤šå±‚Transformerç¼–ç å™¨å¤„ç†")
        print("   - è¾“å‡º: æ–‡æœ¬çš„ç‰¹å¾å‘é‡è¡¨ç¤º")

        print("\n4. å¯¹æ¯”å­¦ä¹ æœºåˆ¶:")
        print("   - è®¡ç®—å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦")
        print("   - æ­£æ ·æœ¬å¯¹(åŒ¹é…çš„å›¾åƒ-æ–‡æœ¬)ç›¸ä¼¼åº¦æœ€å¤§åŒ–")
        print("   - è´Ÿæ ·æœ¬å¯¹(ä¸åŒ¹é…çš„å›¾åƒ-æ–‡æœ¬)ç›¸ä¼¼åº¦æœ€å°åŒ–")

    def demonstrate_clip_components(self):
        """æ¼”ç¤ºCLIPå„ä¸ªç»„ä»¶çš„å·¥ä½œåŸç†"""
        print("\n" + "=" * 60)
        print("CLIP ç»„ä»¶æ¼”ç¤º")
        print("=" * 60)

        if hasattr(self, 'demo_mode'):
            print("âš ï¸  å½“å‰ä¸ºæ¼”ç¤ºæ¨¡å¼ï¼Œæ˜¾ç¤ºæ¨¡æ‹Ÿæ•°æ®")

        # 1. å›¾åƒç¼–ç å™¨æ¼”ç¤º
        print("\n1. å›¾åƒç¼–ç å™¨å·¥ä½œæµç¨‹:")
        dummy_image = torch.randn(1, 3, 224, 224)
        print(f"   è¾“å…¥å›¾åƒshape: {dummy_image.shape}")

        vision_model = self.model.vision_model
        print(f"   Vision Transformerå±‚æ•°: {len(vision_model.encoder.layers)}")
        print(f"   éšè—å±‚ç»´åº¦: {vision_model.config.hidden_size}")
        print(f"   æ³¨æ„åŠ›å¤´æ•°: {vision_model.config.num_attention_heads}")
        print(f"   å›¾åƒå°ºå¯¸: {vision_model.config.image_size}")
        print(f"   Patchå°ºå¯¸: {vision_model.config.patch_size}")

        # 2. æ–‡æœ¬ç¼–ç å™¨æ¼”ç¤º
        print("\n2. æ–‡æœ¬ç¼–ç å™¨å·¥ä½œæµç¨‹:")
        dummy_text = torch.randint(0, 1000, (1, 77))
        print(f"   è¾“å…¥æ–‡æœ¬tokens shape: {dummy_text.shape}")

        text_model = self.model.text_model
        print(f"   Text Transformerå±‚æ•°: {len(text_model.encoder.layers)}")
        print(f"   éšè—å±‚ç»´åº¦: {text_model.config.hidden_size}")
        print(f"   è¯æ±‡è¡¨å¤§å°: {text_model.config.vocab_size}")
        print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {text_model.config.max_position_embeddings}")

        # 3. æŠ•å½±å±‚ä¿¡æ¯
        print("\n3. æŠ•å½±å±‚ä¿¡æ¯:")
        print(f"   è§†è§‰æŠ•å½±ç»´åº¦: {self.model.visual_projection.in_features} â†’ {self.model.visual_projection.out_features}")
        print(f"   æ–‡æœ¬æŠ•å½±ç»´åº¦: {self.model.text_projection.in_features} â†’ {self.model.text_projection.out_features}")
        print(f"   æ¸©åº¦å‚æ•°: {self.model.logit_scale.item():.4f}")

    def detailed_inference_walkthrough(self):
        """è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹æ¼”ç¤º"""
        print("\n" + "=" * 80)
        print("CLIP è¯¦ç»†æ¨ç†è¿‡ç¨‹æ¼”ç¤º")
        print("=" * 80)

        if hasattr(self, 'demo_mode'):
            print("âš ï¸  å½“å‰ä¸ºæ¼”ç¤ºæ¨¡å¼ï¼Œæ˜¾ç¤ºæ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹")
            self._demo_detailed_inference()
            return
        else:
            print("ğŸ‰ ä½¿ç”¨çœŸå®CLIPæ¨¡å‹è¿›è¡Œè¯¦ç»†æ¨ç†")

        # å‡†å¤‡å…·ä½“çš„ç¤ºä¾‹æ•°æ®
        print("\nğŸ“ 1. å‡†å¤‡ç¤ºä¾‹æ•°æ®:")

        # åˆ›å»ºä¸€ä¸ªçº¢è‰²æ–¹å—å›¾åƒ
        image = Image.new('RGB', (224, 224), color=(255, 0, 0))  # çº¯çº¢è‰²
        texts = ["a red square", "a blue circle", "a green triangle"]

        print(f"   å›¾åƒ: 224x224 çº¢è‰²æ–¹å—")
        print(f"   å€™é€‰æ–‡æœ¬: {texts}")

        # è¯¦ç»†çš„æ•°æ®é¢„å¤„ç†
        print(f"\nğŸ” 2. æ•°æ®é¢„å¤„ç†è¯¦ç»†è¿‡ç¨‹:")
        inputs = self.processor(text=texts, images=image, debug=True)

        print(f"\nğŸ§  3. æ¨¡å‹æ¨ç†è¯¦ç»†è¿‡ç¨‹:")

        with torch.no_grad():
            # åˆ†æ­¥éª¤è¿›è¡Œæ¨ç†ï¼Œè·å–ä¸­é—´ç»“æœ
            print(f"   ğŸ–¼ï¸  å›¾åƒç¼–ç è¿‡ç¨‹:")

            # è·å–å›¾åƒç‰¹å¾
            vision_outputs = self.model.vision_model(pixel_values=inputs['pixel_values'])
            image_features_raw = vision_outputs.last_hidden_state[:, 0, :]  # CLS token

            print(f"     Vision Transformerè¾“å‡º shape: {vision_outputs.last_hidden_state.shape}")
            print(f"     CLS tokenç‰¹å¾ shape: {image_features_raw.shape}")
            print(f"     CLS tokenå‰5ç»´: {image_features_raw[0, :5].detach().numpy()}")

            # å›¾åƒæŠ•å½±
            image_embeds = self.model.visual_projection(image_features_raw)
            print(f"     æŠ•å½±åå›¾åƒç‰¹å¾ shape: {image_embeds.shape}")
            print(f"     æŠ•å½±åå‰5ç»´: {image_embeds[0, :5].detach().numpy()}")

            # L2å½’ä¸€åŒ–
            image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)
            print(f"     å½’ä¸€åŒ–åå‰5ç»´: {image_embeds[0, :5].detach().numpy()}")
            print(f"     ç‰¹å¾å‘é‡é•¿åº¦: {torch.norm(image_embeds[0]).item():.6f}")

            print(f"\n   ğŸ“ æ–‡æœ¬ç¼–ç è¿‡ç¨‹:")

            # è·å–æ–‡æœ¬ç‰¹å¾
            text_outputs = self.model.text_model(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask']
            )

            # è·å–æœ€åä¸€ä¸ªtokençš„ç‰¹å¾ï¼ˆé€šå¸¸æ˜¯EOS tokenï¼‰
            sequence_lengths = inputs['attention_mask'].sum(dim=1) - 1
            text_features_raw = text_outputs.last_hidden_state[
                torch.arange(text_outputs.last_hidden_state.shape[0]),
                sequence_lengths
            ]

            print(f"     Text Transformerè¾“å‡º shape: {text_outputs.last_hidden_state.shape}")
            print(f"     æœ€åtokenä½ç½®: {sequence_lengths.detach().numpy()}")
            print(f"     æ–‡æœ¬ç‰¹å¾ shape: {text_features_raw.shape}")

            for i, text in enumerate(texts):
                print(f"     æ–‡æœ¬{i} '{text}' ç‰¹å¾å‰3ç»´: {text_features_raw[i, :3].detach().numpy()}")

            # æ–‡æœ¬æŠ•å½±
            text_embeds = self.model.text_projection(text_features_raw)
            print(f"     æŠ•å½±åæ–‡æœ¬ç‰¹å¾ shape: {text_embeds.shape}")

            # L2å½’ä¸€åŒ–
            text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)
            print(f"     å½’ä¸€åŒ–åæ–‡æœ¬ç‰¹å¾:")
            for i, text in enumerate(texts):
                print(f"       '{text}' å‰3ç»´: {text_embeds[i, :3].detach().numpy()}")
                print(f"       å‘é‡é•¿åº¦: {torch.norm(text_embeds[i]).item():.6f}")

            print(f"\n   ğŸ”— ç›¸ä¼¼åº¦è®¡ç®—è¿‡ç¨‹:")

            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            logit_scale = self.model.logit_scale.exp()
            print(f"     æ¸©åº¦å‚æ•° exp(logit_scale): {logit_scale.item():.6f}")

            # ç‚¹ç§¯è®¡ç®—
            similarity_raw = torch.matmul(image_embeds, text_embeds.t())
            print(f"     åŸå§‹ç‚¹ç§¯ç›¸ä¼¼åº¦:")
            for i, text in enumerate(texts):
                print(f"       ä¸ '{text}': {similarity_raw[0, i].item():.6f}")

            # æ¸©åº¦ç¼©æ”¾
            logits_per_image = logit_scale * similarity_raw
            print(f"     æ¸©åº¦ç¼©æ”¾ålogits:")
            for i, text in enumerate(texts):
                print(f"       ä¸ '{text}': {logits_per_image[0, i].item():.6f}")

            # Softmaxæ¦‚ç‡
            probs = F.softmax(logits_per_image, dim=-1)
            print(f"     Softmaxæ¦‚ç‡åˆ†å¸ƒ:")
            for i, text in enumerate(texts):
                print(f"       '{text}': {probs[0, i].item():.6f}")

            # éªŒè¯æ¦‚ç‡å’Œ
            print(f"     æ¦‚ç‡å’Œ: {probs.sum().item():.6f}")

            print(f"\nğŸ“Š 4. æœ€ç»ˆç»“æœåˆ†æ:")

            # æ‰¾åˆ°æœ€ä½³åŒ¹é…
            best_match_idx = torch.argmax(probs, dim=-1)
            best_text = texts[best_match_idx[0]]
            best_prob = probs[0, best_match_idx[0]].item()

            print(f"   æœ€ä½³åŒ¹é…: '{best_text}' (æ¦‚ç‡: {best_prob:.4f})")

            # è®¡ç®—ç½®ä¿¡åº¦
            sorted_probs, sorted_indices = torch.sort(probs[0], descending=True)
            confidence = sorted_probs[0] - sorted_probs[1]
            print(f"   ç½®ä¿¡åº¦ (æœ€é«˜-æ¬¡é«˜): {confidence.item():.4f}")

            # æ˜¾ç¤ºæ’åºç»“æœ
            print(f"   å®Œæ•´æ’åº:")
            for i, idx in enumerate(sorted_indices):
                text = texts[idx]
                prob = sorted_probs[i].item()
                print(f"     {i+1}. '{text}': {prob:.4f}")

    def _demo_detailed_inference(self):
        """æ¼”ç¤ºæ¨¡å¼çš„è¯¦ç»†æ¨ç†"""
        print("ğŸ­ æ¼”ç¤ºæ¨¡å¼è¯¦ç»†æ¨ç†è¿‡ç¨‹:")

        # æ¨¡æ‹Ÿæ•°æ®
        texts = ["a red square", "a blue circle", "a green triangle"]

        print(f"\nğŸ“ æ¨¡æ‹Ÿæ•°æ®:")
        print(f"   å›¾åƒ: 224x224 çº¢è‰²æ–¹å—")
        print(f"   å€™é€‰æ–‡æœ¬: {texts}")

        # æ¨¡æ‹Ÿç‰¹å¾
        image_features = torch.randn(1, 512)
        text_features = torch.randn(len(texts), 512)

        # å½’ä¸€åŒ–
        image_features = F.normalize(image_features, dim=-1)
        text_features = F.normalize(text_features, dim=-1)

        print(f"\nğŸ§  æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹:")
        print(f"   å›¾åƒç‰¹å¾ shape: {image_features.shape}")
        print(f"   æ–‡æœ¬ç‰¹å¾ shape: {text_features.shape}")

        # ç›¸ä¼¼åº¦è®¡ç®—
        similarity = torch.matmul(image_features, text_features.t())
        logit_scale = 2.6593  # å…¸å‹çš„CLIPæ¸©åº¦å‚æ•°
        logits = logit_scale * similarity
        probs = F.softmax(logits, dim=-1)

        print(f"   ç›¸ä¼¼åº¦è®¡ç®—:")
        for i, text in enumerate(texts):
            print(f"     '{text}': ç›¸ä¼¼åº¦={similarity[0,i].item():.4f}, æ¦‚ç‡={probs[0,i].item():.4f}")

    def clip_inference_example(self):
        """CLIPæ¨ç†è¿‡ç¨‹çš„å®Œæ•´ç¤ºä¾‹"""
        print("\n" + "=" * 60)
        print("CLIP æ¨ç†ç¤ºä¾‹")
        print("=" * 60)

        if hasattr(self, 'demo_mode'):
            print("âš ï¸  å½“å‰ä¸ºæ¼”ç¤ºæ¨¡å¼ï¼Œæ˜¾ç¤ºæ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹")
        else:
            print("ğŸ‰ ä½¿ç”¨çœŸå®CLIPæ¨¡å‹è¿›è¡Œæ¨ç†")

        # å‡†å¤‡ç¤ºä¾‹æ•°æ®
        image = Image.new('RGB', (224, 224), color='red')
        texts = ["a red image", "a blue image", "a green image", "a cat", "a dog"]

        print("\n1. æ•°æ®é¢„å¤„ç†:")
        print(f"   å›¾åƒå°ºå¯¸: {image.size}")
        print(f"   å€™é€‰æ–‡æœ¬: {texts}")

        # ä½¿ç”¨processorå¤„ç†è¾“å…¥
        inputs = self.processor(text=texts, images=image, return_tensors="pt", padding=True)
        print(f"   å¤„ç†åçš„å›¾åƒtensor shape: {inputs['pixel_values'].shape}")
        print(f"   å¤„ç†åçš„æ–‡æœ¬tokens shape: {inputs['input_ids'].shape}")
        print(f"   æ³¨æ„åŠ›mask shape: {inputs['attention_mask'].shape}")

        print("\n2. æ¨¡å‹æ¨ç†è¿‡ç¨‹:")

        if hasattr(self, 'demo_mode'):
            # æ¼”ç¤ºæ¨¡å¼ï¼šåˆ›å»ºæ¨¡æ‹Ÿè¾“å‡º
            print("   ğŸ”§ ç”Ÿæˆæ¨¡æ‹Ÿæ¨ç†ç»“æœ...")

            # æ¨¡æ‹Ÿç‰¹å¾
            image_features = torch.randn(1, 512)
            text_features = torch.randn(len(texts), 512)

            print(f"   å›¾åƒç‰¹å¾ç»´åº¦: {image_features.shape}")
            print(f"   æ–‡æœ¬ç‰¹å¾ç»´åº¦: {text_features.shape}")

            # æ¨¡æ‹Ÿç›¸ä¼¼åº¦è®¡ç®—
            similarity_matrix = torch.matmul(image_features, text_features.t())
            print(f"   å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ: {similarity_matrix.shape}")

            # æ¨¡æ‹Ÿæ¦‚ç‡åˆ†å¸ƒ
            probs = F.softmax(similarity_matrix, dim=-1)

            print("\n3. æ¨ç†ç»“æœ (æ¨¡æ‹Ÿ):")
            for i, text in enumerate(texts):
                print(f"   '{text}': {probs[0][i].item():.4f}")
        else:
            # çœŸå®æ¨¡å¼ï¼šä½¿ç”¨å®é™…æ¨¡å‹
            with torch.no_grad():
                outputs = self.model(**inputs)

                image_features = outputs.image_embeds
                text_features = outputs.text_embeds

                print(f"   å›¾åƒç‰¹å¾ç»´åº¦: {image_features.shape}")
                print(f"   æ–‡æœ¬ç‰¹å¾ç»´åº¦: {text_features.shape}")

                logits_per_image = outputs.logits_per_image
                print(f"   å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ: {logits_per_image.shape}")

                probs = logits_per_image.softmax(dim=-1)

                print("\n3. æ¨ç†ç»“æœ (çœŸå®æ¨¡å‹):")
                for i, text in enumerate(texts):
                    print(f"   '{text}': {probs[0][i].item():.4f}")

    def analyze_attention_patterns(self):
        """åˆ†ææ³¨æ„åŠ›æ¨¡å¼ï¼ˆå¦‚æœå¯èƒ½çš„è¯ï¼‰"""
        print("\n" + "=" * 60)
        print("CLIP æ³¨æ„åŠ›æ¨¡å¼åˆ†æ")
        print("=" * 60)

        if hasattr(self, 'demo_mode'):
            print("âš ï¸  æ¼”ç¤ºæ¨¡å¼ï¼šæ— æ³•è·å–çœŸå®æ³¨æ„åŠ›æƒé‡")
            print("ğŸ’¡ æ³¨æ„åŠ›æœºåˆ¶è¯´æ˜:")
            print("   - Vision Transformerä¸­ï¼Œæ¯ä¸ªpatchéƒ½ä¼šä¸å…¶ä»–patchäº¤äº’")
            print("   - CLS tokené€šè¿‡æ³¨æ„åŠ›æœºåˆ¶èšåˆæ‰€æœ‰patchçš„ä¿¡æ¯")
            print("   - æ–‡æœ¬Transformerä¸­ï¼Œæ¯ä¸ªtokenå…³æ³¨ä¸Šä¸‹æ–‡ä¿¡æ¯")
            print("   - æ³¨æ„åŠ›æƒé‡åæ˜ äº†æ¨¡å‹å…³æ³¨çš„é‡ç‚¹åŒºåŸŸ")
            return

        print("ğŸ” å°è¯•åˆ†ææ³¨æ„åŠ›æ¨¡å¼...")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        image = Image.new('RGB', (224, 224), color='red')
        text = "a red image"

        inputs = self.processor(text=text, images=image)

        try:
            # å°è¯•è·å–æ³¨æ„åŠ›æƒé‡ï¼ˆéœ€è¦æ¨¡å‹æ”¯æŒè¾“å‡ºæ³¨æ„åŠ›ï¼‰
            with torch.no_grad():
                outputs = self.model(**inputs, output_attentions=True)

            if hasattr(outputs, 'vision_model_output') and hasattr(outputs.vision_model_output, 'attentions'):
                vision_attentions = outputs.vision_model_output.attentions
                print(f"   Visionæ³¨æ„åŠ›å±‚æ•°: {len(vision_attentions)}")
                print(f"   æœ€åä¸€å±‚æ³¨æ„åŠ› shape: {vision_attentions[-1].shape}")

                # åˆ†æCLS tokençš„æ³¨æ„åŠ›
                cls_attention = vision_attentions[-1][0, :, 0, :]  # [num_heads, num_patches+1]
                print(f"   CLS tokenæ³¨æ„åŠ›æƒé‡ shape: {cls_attention.shape}")

                # æ˜¾ç¤ºæ¯ä¸ªå¤´çš„æ³¨æ„åŠ›åˆ†å¸ƒ
                for head in range(min(3, cls_attention.shape[0])):  # åªæ˜¾ç¤ºå‰3ä¸ªå¤´
                    attention_weights = cls_attention[head]
                    print(f"   æ³¨æ„åŠ›å¤´{head}:")
                    print(f"     å¯¹è‡ªå·±(CLS): {attention_weights[0].item():.4f}")
                    print(f"     å¯¹patchçš„å¹³å‡æ³¨æ„åŠ›: {attention_weights[1:].mean().item():.4f}")
                    print(f"     æœ€å¤§æ³¨æ„åŠ›patch: {attention_weights[1:].argmax().item()}")

            if hasattr(outputs, 'text_model_output') and hasattr(outputs.text_model_output, 'attentions'):
                text_attentions = outputs.text_model_output.attentions
                print(f"   Textæ³¨æ„åŠ›å±‚æ•°: {len(text_attentions)}")
                print(f"   æœ€åä¸€å±‚æ³¨æ„åŠ› shape: {text_attentions[-1].shape}")

        except Exception as e:
            print(f"   âš ï¸  æ— æ³•è·å–æ³¨æ„åŠ›æƒé‡: {e}")
            print("   ğŸ’¡ å¯èƒ½éœ€è¦åœ¨æ¨¡å‹è°ƒç”¨æ—¶è®¾ç½® output_attentions=True")

def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡ŒCLIPè¯¦ç»†è®²è§£"""
    print("CLIP (Contrastive Language-Image Pre-training) è¯¦ç»†è®²è§£")
    print("ğŸš€ ç›´æ¥åŠ è½½ç‰ˆæœ¬ - åŒ…å«è¯¦ç»†ä¸­é—´é‡åˆ†æ")
    print("=" * 80)

    try:
        # åˆ›å»ºè®²è§£å®ä¾‹
        clip_demo = CLIPExplanation()

        # 1. æ¶æ„è®²è§£
        clip_demo.explain_clip_architecture()

        # 2. ç»„ä»¶æ¼”ç¤º
        clip_demo.demonstrate_clip_components()

        # 3. è¯¦ç»†æ¨ç†è¿‡ç¨‹æ¼”ç¤º
        clip_demo.detailed_inference_walkthrough()

        # 4. æ³¨æ„åŠ›æ¨¡å¼åˆ†æ
        clip_demo.analyze_attention_patterns()

        # 5. åŸºç¡€æ¨ç†ç¤ºä¾‹
        clip_demo.clip_inference_example()

        print("\n" + "=" * 80)
        print("âœ… CLIP è¯¦ç»†è®²è§£å®Œæˆï¼")
        if hasattr(clip_demo, 'demo_mode'):
            print("ğŸ“ å½“å‰ä¸ºæ¼”ç¤ºæ¨¡å¼ï¼Œå·²å±•ç¤ºCLIPçš„æ ¸å¿ƒæ¦‚å¿µå’Œæ¶æ„")
            print("ğŸ’¡ æ¨¡å‹æ–‡ä»¶å·²ä¸‹è½½ï¼Œä½†ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜å¯¼è‡´æ— æ³•åŠ è½½")
            print("ğŸ”§ å»ºè®®é™çº§transformersç‰ˆæœ¬: pip install transformers==4.21.0")
        else:
            print("ğŸ‰ æˆåŠŸä½¿ç”¨çœŸå®CLIPæ¨¡å‹ï¼")
            print("ğŸ’¾ æ¨¡å‹å·²æˆåŠŸåŠ è½½å¹¶ç¼“å­˜åˆ°æœ¬åœ°")
            print("ğŸ” å·²å±•ç¤ºè¯¦ç»†çš„ä¸­é—´é‡è®¡ç®—è¿‡ç¨‹")
        print("=" * 80)

    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
