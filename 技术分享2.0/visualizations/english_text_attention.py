"""
ä¸“é—¨é’ˆå¯¹è‹±æ–‡æ–‡æœ¬"two dogs lying on a cushion in the sun"çš„æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–
ä½¿ç”¨çœŸå®çš„CLIPæ¨¡å‹æ•°æ®ï¼Œä¸åšè¿‡å¤šå®¹é”™å¤„ç†
"""

from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def visualize_english_text_attention(attention_data, output_dir="outputs"):
    """
    ä¸“é—¨å¯è§†åŒ–è‹±æ–‡æ–‡æœ¬"two dogs lying on a cushion in the sun"çš„æ³¨æ„åŠ›æœºåˆ¶

    Args:
        attention_data: æ³¨æ„åŠ›æ•°æ®ï¼Œå¿…é¡»åŒ…å«çœŸå®çš„text_attentions
        output_dir: è¾“å‡ºç›®å½•
    """
    print("ç”Ÿæˆè‹±æ–‡æ–‡æœ¬æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–...")

    Path(output_dir).mkdir(exist_ok=True)

    # ç›®æ ‡æ–‡æœ¬
    target_text = "two dogs lying on a cushion in the sun"

    # å¯»æ‰¾ç›®æ ‡æ–‡æœ¬
    target_idx = 0
    for i, text in enumerate(attention_data['texts']):
        if target_text.lower() in text.lower():
            target_idx = i
            break

    print(f"æ‰¾åˆ°ç›®æ ‡æ–‡æœ¬: {attention_data['texts'][target_idx]}")

    # è·å–çœŸå®æ•°æ®
    text_attentions = attention_data['text_attentions']
    text_inputs = attention_data['text_inputs']

    input_ids = text_inputs['input_ids'][target_idx].numpy()
    attention_mask = text_inputs['attention_mask'][target_idx].numpy()
    valid_length = int(attention_mask.sum())

    print(f"Tokenåºåˆ—é•¿åº¦: {valid_length}")
    print(f"Input IDs: {input_ids[:valid_length]}")

    # è¿‡æ»¤æ‰CLSå’ŒSEP tokenï¼Œåªä¿ç•™å†…å®¹token
    content_start = 1  # è·³è¿‡CLS
    content_end = valid_length - 1  # è·³è¿‡SEP
    display_length = min(content_end - content_start, 9)  # æœ€å¤šæ˜¾ç¤º9ä¸ªå†…å®¹token

    # è‹±æ–‡å†…å®¹tokensï¼ˆé¢„æœŸçš„åˆ†è¯ç»“æœï¼‰
    expected_tokens = ['two', 'dogs', 'lying', 'on', 'a', 'cushion', 'in', 'the', 'sun']

    # åˆ›å»ºtokenæ ‡ç­¾
    token_labels = expected_tokens[:display_length]

    # è·å–æœ€åä¸€å±‚çš„æ³¨æ„åŠ›å¹¶è¿‡æ»¤
    final_attention = text_attentions[-1][target_idx]  # [num_heads, seq_len, seq_len]
    avg_attention = final_attention.mean(dim=0).numpy()  # å¹³å‡æ‰€æœ‰å¤´
    attention_matrix = avg_attention[content_start:content_start+display_length,
                                   content_start:content_start+display_length]

    print(f"æ³¨æ„åŠ›çŸ©é˜µå½¢çŠ¶: {attention_matrix.shape}")
    print(f"æ˜¾ç¤ºtokens: {token_labels}")

    # åˆ›å»ºå¤§å›¾ï¼ŒåŒ…å«å¤šä¸ªå­å›¾
    fig = plt.figure(figsize=(20, 16))
    fig.suptitle(f'English Text Attention Analysis\n"{target_text}"',
                 fontsize=18, fontweight='bold')

    # 1. Tokenåºåˆ—å±•ç¤º (é¡¶éƒ¨)
    ax1 = plt.subplot2grid((4, 4), (0, 0), colspan=4)
    ax1.set_title('Content Token Sequence (CLS/SEP Filtered)', fontsize=14, fontweight='bold')

    # ç»˜åˆ¶tokenåºåˆ—
    colors = ['lightblue', 'lightcoral', 'lightgreen', 'lightyellow', 'lightpink',
              'lightgray', 'lightcyan', 'wheat', 'lavender']

    for i, (token, color) in enumerate(zip(token_labels, colors)):
        rect = plt.Rectangle((i, 0), 1, 1, facecolor=color, edgecolor='black', linewidth=2)
        ax1.add_patch(rect)
        ax1.text(i+0.5, 0.5, token, ha='center', va='center', fontweight='bold', fontsize=12)
        ax1.text(i+0.5, -0.3, f'ID:{input_ids[content_start + i]}', ha='center', va='center', fontsize=8)

    ax1.set_xlim(0, display_length)
    ax1.set_ylim(-0.5, 1.2)
    ax1.set_aspect('equal')
    ax1.axis('off')

    # 2. æ³¨æ„åŠ›çƒ­åŠ›å›¾ (å·¦ä¸Š)
    ax2 = plt.subplot2grid((4, 4), (1, 0), colspan=2, rowspan=2)

    im = ax2.imshow(attention_matrix, cmap='Blues', interpolation='nearest')
    ax2.set_title('Token-to-Token Attention Matrix', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Key Token', fontsize=12)
    ax2.set_ylabel('Query Token', fontsize=12)
    ax2.set_xticks(range(display_length))
    ax2.set_yticks(range(display_length))
    ax2.set_xticklabels(token_labels, rotation=45, ha='right')
    ax2.set_yticklabels(token_labels)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i in range(display_length):
        for j in range(display_length):
            value = attention_matrix[i, j]
            color = 'white' if value > 0.5 else 'black'
            ax2.text(j, i, f'{value:.2f}', ha='center', va='center',
                    color=color, fontsize=9, fontweight='bold')

    plt.colorbar(im, ax=ax2, shrink=0.8)

    # 3. å…³é”®è¯"dogs"çš„æ³¨æ„åŠ›åˆ†å¸ƒ (å³ä¸Š)
    ax3 = plt.subplot2grid((4, 4), (1, 2), colspan=2)

    dogs_idx = 1  # "dogs"åœ¨å†…å®¹tokenä¸­çš„ä½ç½®
    key_attention = attention_matrix[dogs_idx, :]

    bars = ax3.bar(range(display_length), key_attention,
                  color=['red' if i == dogs_idx else 'lightblue' for i in range(display_length)],
                  alpha=0.7)
    ax3.set_title('Key Token "dogs" - Attention Distribution', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Target Token')
    ax3.set_ylabel('Attention Weight')
    ax3.set_xticks(range(display_length))
    ax3.set_xticklabels(token_labels, rotation=45)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars, key_attention):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{value:.2f}', ha='center', va='bottom', fontsize=9)

    # 4. è‡ªæ³¨æ„åŠ›å¼ºåº¦ (å·¦ä¸‹)
    ax4 = plt.subplot2grid((4, 4), (2, 2))

    self_attention = np.diag(attention_matrix)
    bars = ax4.bar(range(display_length), self_attention, color='orange', alpha=0.7)
    ax4.set_title('Self-Attention Strength', fontsize=12, fontweight='bold')
    ax4.set_xlabel('Token')
    ax4.set_ylabel('Self-Attention')
    ax4.set_xticks(range(display_length))
    ax4.set_xticklabels(token_labels, rotation=45)

    # 5. æ³¨æ„åŠ›ç½‘ç»œå›¾ (å³ä¸‹)
    ax5 = plt.subplot2grid((4, 4), (2, 3))

    # åˆ›å»ºåœ†å½¢å¸ƒå±€çš„æ³¨æ„åŠ›ç½‘ç»œ
    threshold = 0.15  # åªæ˜¾ç¤ºæ³¨æ„åŠ›æƒé‡å¤§äºé˜ˆå€¼çš„è¿æ¥

    positions = {}
    for i in range(display_length):
        angle = 2 * np.pi * i / display_length
        x = np.cos(angle)
        y = np.sin(angle)
        positions[i] = (x, y)

        # èŠ‚ç‚¹å¤§å°æ ¹æ®æ€»æ³¨æ„åŠ›æƒé‡
        node_size = attention_matrix[i, :].sum() * 800
        color = 'red' if i == dogs_idx else 'lightblue'
        ax5.scatter(x, y, s=node_size, alpha=0.7, c=color)
        ax5.text(x*1.3, y*1.3, token_labels[i], ha='center', va='center', fontsize=10, fontweight='bold')

    # ç»˜åˆ¶æ³¨æ„åŠ›è¿æ¥
    for i in range(display_length):
        for j in range(display_length):
            if i != j and attention_matrix[i, j] > threshold:
                x1, y1 = positions[i]
                x2, y2 = positions[j]
                alpha = attention_matrix[i, j]
                ax5.plot([x1, x2], [y1, y2], 'gray', alpha=alpha, linewidth=alpha*4)

    ax5.set_title('Attention Network', fontsize=12, fontweight='bold')
    ax5.set_xlim(-1.8, 1.8)
    ax5.set_ylim(-1.8, 1.8)
    ax5.axis('off')

    # 6. å¤šå¤´æ³¨æ„åŠ›åˆ†æ (åº•éƒ¨å·¦)
    ax6 = plt.subplot2grid((4, 4), (3, 0), colspan=2)

    num_heads = min(4, final_attention.shape[0])
    head_patterns = []

    for head in range(num_heads):
        head_att = final_attention[head, content_start:content_start+display_length,
                                  content_start:content_start+display_length].numpy()
        diag_strength = np.diag(head_att).mean()
        off_diag_strength = (head_att.sum() - np.diag(head_att).sum()) / (display_length*display_length - display_length)
        head_patterns.append([diag_strength, off_diag_strength])

    head_patterns = np.array(head_patterns)
    x = np.arange(num_heads)
    width = 0.35

    ax6.bar(x - width/2, head_patterns[:, 0], width, label='Self-Attention', alpha=0.8, color='orange')
    ax6.bar(x + width/2, head_patterns[:, 1], width, label='Cross-Attention', alpha=0.8, color='lightblue')
    ax6.set_title('Multi-Head Attention Patterns', fontsize=12, fontweight='bold')
    ax6.set_xlabel('Attention Head')
    ax6.set_ylabel('Attention Strength')
    ax6.set_xticks(x)
    ax6.set_xticklabels([f'Head{i+1}' for i in range(num_heads)])
    ax6.legend()
    ax6.grid(True, alpha=0.3)

    # 7. è¯¦ç»†åˆ†ææŠ¥å‘Š (åº•éƒ¨å³)
    ax7 = plt.subplot2grid((4, 4), (3, 2), colspan=2)
    ax7.axis('off')

    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    most_attended_by_dogs = np.argmax(attention_matrix[dogs_idx, :])
    strongest_self_attention = np.argmax(self_attention)
    max_cross_attention = attention_matrix.max()
    avg_attention = attention_matrix.mean()

    report_text = f"""ATTENTION ANALYSIS REPORT

Text: "{target_text}"
Tokens Analyzed: {display_length} content tokens

KEY FINDINGS:
â€¢ "dogs" pays most attention to: "{token_labels[most_attended_by_dogs]}"
â€¢ Strongest self-attention: "{token_labels[strongest_self_attention]}"
â€¢ Maximum cross-attention: {max_cross_attention:.3f}
â€¢ Average attention weight: {avg_attention:.3f}

TOKEN BREAKDOWN:
"""

    for i, (token, token_id) in enumerate(zip(token_labels, input_ids[content_start:content_start+display_length])):
        report_text += f"[{i}] {token} (ID: {token_id})\n"

    ax7.text(0.05, 0.95, report_text, transform=ax7.transAxes, fontsize=11,
            verticalalignment='top', fontfamily='monospace',
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))

    plt.tight_layout()

    # ä¿å­˜å›¾åƒ
    output_path = Path(output_dir) / "english_text_attention_analysis.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"âœ“ è‹±æ–‡æ–‡æœ¬æ³¨æ„åŠ›åˆ†æå®Œæˆ")
    print(f"âœ“ åˆ†æäº† {display_length} ä¸ªå†…å®¹tokensï¼ˆå·²è¿‡æ»¤CLS/SEPï¼‰")
    print(f"âœ“ å¯è§†åŒ–å·²ä¿å­˜: {output_path}")

    return output_path

def run_english_text_attention_demo():
    """
    è‡ªé—­ç¯è¿è¡Œè‹±æ–‡æ–‡æœ¬æ³¨æ„åŠ›åˆ†ææ¼”ç¤º
    ç›´æ¥åŠ è½½æ¨¡å‹ï¼Œå¤„ç†"two dogs lying on a cushion in the sun"è¿™å¥è¯
    """
    print("=" * 60)
    print("è‹±æ–‡æ–‡æœ¬æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–æ¼”ç¤º")
    print("ç›®æ ‡æ–‡æœ¬: 'two dogs lying on a cushion in the sun'")
    print("=" * 60)

    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent))

    from utils.model_loader import load_local_clip_model
    from utils.attention_extractor import extract_attention_weights
    from PIL import Image
    import torch

    # è®¾ç½®è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. åŠ è½½æ¨¡å‹
    print("åŠ è½½CLIPæ¨¡å‹...")
    model, tokenizer, model_loaded = load_local_clip_model()
    if not model_loaded:
        print("âŒ æ— æ³•åŠ è½½æ¨¡å‹")
        return

    model = model.to(device).eval()
    print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")

    # 2. åŠ è½½å›¾åƒ
    img_path = Path(__file__).parent.parent / "dogs_sun_patio.jpeg"
    if not img_path.exists():
        print(f"âŒ å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
        return

    image = Image.open(img_path).convert('RGB')
    print("âœ“ å›¾åƒåŠ è½½æˆåŠŸ")

    # 3. å‡†å¤‡æ–‡æœ¬
    target_text = "two dogs lying on a cushion in the sun"
    texts = [target_text]  # åªåˆ†æè¿™ä¸€å¥è¯
    print(f"âœ“ ç›®æ ‡æ–‡æœ¬: {target_text}")

    # 4. æå–æ³¨æ„åŠ›æ•°æ®
    print("æå–æ³¨æ„åŠ›æƒé‡...")
    attention_data = extract_attention_weights(model, tokenizer, image, texts, device)

    if not attention_data:
        print("âŒ æ³¨æ„åŠ›æ•°æ®æå–å¤±è´¥")
        return

    print("âœ“ æ³¨æ„åŠ›æ•°æ®æå–æˆåŠŸ")
    print(f"  - æ–‡æœ¬æ³¨æ„åŠ›å±‚æ•°: {len(attention_data.get('text_attentions', []))}")
    print(f"  - æ–‡æœ¬é•¿åº¦: {len(attention_data.get('texts', []))}")

    # 5. ç”Ÿæˆå¯è§†åŒ–
    output_dir = Path(__file__).parent.parent / "outputs" / "english_text_attention"
    print(f"ç”Ÿæˆå¯è§†åŒ–åˆ°: {output_dir}")

    result_path = visualize_english_text_attention(attention_data, str(output_dir))

    print("=" * 60)
    print("âœ… è‹±æ–‡æ–‡æœ¬æ³¨æ„åŠ›åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {result_path}")
    print("=" * 60)

    return result_path

if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæ¼”ç¤º
    run_english_text_attention_demo()

