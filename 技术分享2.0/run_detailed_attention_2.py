#!/usr/bin/env python3
"""
è‹±æ–‡æ–‡æœ¬æ³¨æ„åŠ›æœºåˆ¶è¿‡ç¨‹å¯è§†åŒ– v2.0
æŒ‰ç…§ï¼šåˆ†è¯ â†’ æŸ¥è¯¢å‘é‡ â†’ ä½ç½®ç¼–ç  â†’ transformer â†’ å¥å­è¯­ä¹‰å‘é‡ çš„è·¯å¾„è¿›è¡Œè¯¦ç»†åˆ†æ
ä¸“é—¨é’ˆå¯¹ "two dogs lying on a cushion in the sun" è¿™å¥è¯
"""

from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from utils.model_loader import load_local_clip_model
from utils.attention_extractor import extract_attention_weights

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def step1_tokenization_analysis(tokenizer, target_text, output_dir):
    """
    æ­¥éª¤1: åˆ†è¯è¿‡ç¨‹åˆ†æ
    """
    print("=" * 50)
    print("æ­¥éª¤1: åˆ†è¯è¿‡ç¨‹åˆ†æ")
    print("=" * 50)

    # è¿›è¡Œåˆ†è¯
    text_inputs = tokenizer(
        target_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=77
    )

    input_ids = text_inputs['input_ids'][0].numpy()
    attention_mask = text_inputs['attention_mask'][0].numpy()
    valid_length = int(attention_mask.sum())

    print(f"åŸå§‹æ–‡æœ¬: {target_text}")
    print(f"Tokenåºåˆ—é•¿åº¦: {valid_length}")
    print(f"Input IDs: {input_ids[:valid_length]}")

    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(3, 1, figsize=(16, 12))
    fig.suptitle('Step 1: Tokenization Process Analysis\n"two dogs lying on a cushion in the sun"',
                 fontsize=16, fontweight='bold')

    # 1.1 åŸå§‹æ–‡æœ¬å±•ç¤º
    axes[0].set_title('1.1 Original Text', fontsize=14, fontweight='bold')
    axes[0].text(0.5, 0.5, f'"{target_text}"', ha='center', va='center',
                 fontsize=18, fontweight='bold',
                 bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue"))
    axes[0].set_xlim(0, 1)
    axes[0].set_ylim(0, 1)
    axes[0].axis('off')

    # 1.2 Token IDåºåˆ—
    axes[1].set_title('1.2 Token ID Sequence', fontsize=14, fontweight='bold')

    # ç»˜åˆ¶token IDæ¡å½¢å›¾
    token_positions = range(valid_length)
    bars = axes[1].bar(token_positions, input_ids[:valid_length],
                       color=['red' if i == 0 else 'orange' if i == valid_length - 1 else 'lightblue'
                              for i in range(valid_length)], alpha=0.7)

    axes[1].set_xlabel('Token Position')
    axes[1].set_ylabel('Token ID')
    axes[1].set_xticks(token_positions)

    # æ·»åŠ token IDæ ‡ç­¾
    for i, (bar, token_id) in enumerate(zip(bars, input_ids[:valid_length])):
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width() / 2., height + 100,
                     f'{token_id}', ha='center', va='bottom', fontsize=10, fontweight='bold')

    # 1.3 Tokenç±»å‹è¯´æ˜
    axes[2].set_title('1.3 Token Type Analysis', fontsize=14, fontweight='bold')

    # é¢„æœŸçš„tokenæ ‡ç­¾
    expected_tokens = ['[CLS]', 'two', 'dogs', 'lying', 'on', 'a', 'cushion', 'in', 'the', 'sun', '[SEP]']
    token_labels = expected_tokens[:valid_length]

    # ç»˜åˆ¶tokenç±»å‹
    colors = ['red', 'lightcoral', 'lightgreen', 'lightyellow', 'lightpink',
              'lightgray', 'lightcyan', 'wheat', 'lavender', 'lightsteelblue', 'orange']

    for i, (token, color) in enumerate(zip(token_labels, colors)):
        rect = plt.Rectangle((i, 0), 1, 1, facecolor=color, edgecolor='black', linewidth=2)
        axes[2].add_patch(rect)
        axes[2].text(i + 0.5, 0.5, token, ha='center', va='center', fontweight='bold', fontsize=11)
        axes[2].text(i + 0.5, -0.3, f'ID:{input_ids[i]}', ha='center', va='center', fontsize=9)

    axes[2].set_xlim(0, valid_length)
    axes[2].set_ylim(-0.5, 1.2)
    axes[2].set_xlabel('Token Position')
    axes[2].axis('off')

    # æ·»åŠ è¯´æ˜
    explanation = f"""åˆ†è¯è¿‡ç¨‹è¯´æ˜ï¼š
â€¢ åŸå§‹æ–‡æœ¬è¢«åˆ†è§£ä¸º {valid_length} ä¸ªtokens
â€¢ [CLS]: åˆ†ç±»æ ‡è®°ï¼Œç”¨äºå¥å­çº§åˆ«çš„è¡¨ç¤º
â€¢ [SEP]: åˆ†éš”æ ‡è®°ï¼Œè¡¨ç¤ºå¥å­ç»“æŸ
â€¢ å†…å®¹tokens: {valid_length - 2} ä¸ªå®é™…å•è¯token
â€¢ æ¯ä¸ªtokenéƒ½æœ‰å”¯ä¸€çš„IDï¼Œç”¨äºåç»­çš„embeddingæŸ¥æ‰¾"""

    plt.figtext(0.5, 0.02, explanation, ha='center', fontsize=11,
                bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow"))

    plt.tight_layout()
    plt.savefig(Path(output_dir) / "step1_tokenization.png", dpi=300, bbox_inches='tight')
    plt.close()

    print("âœ“ åˆ†è¯è¿‡ç¨‹åˆ†æå®Œæˆ")
    return text_inputs, token_labels


def step2_query_vector_analysis(model, text_inputs, token_labels, output_dir):
    """
    æ­¥éª¤2: æŸ¥è¯¢å‘é‡ç”Ÿæˆåˆ†æ
    """
    print("=" * 50)
    print("æ­¥éª¤2: æŸ¥è¯¢å‘é‡ç”Ÿæˆåˆ†æ")
    print("=" * 50)

    device = next(model.parameters()).device
    text_inputs_device = {k: v.to(device) for k, v in text_inputs.items()}

    # è·å–embeddingå±‚çš„è¾“å‡º
    with torch.no_grad():
        # è·å–token embeddings
        token_embeddings = model.text_model.embeddings.token_embedding(text_inputs_device['input_ids'])
        print(f"Token embeddings shape: {token_embeddings.shape}")  # [1, seq_len, hidden_size]

        # è·å–ä½ç½®embeddings
        position_ids = torch.arange(text_inputs_device['input_ids'].shape[1], device=device).unsqueeze(0)
        position_embeddings = model.text_model.embeddings.position_embedding(position_ids)
        print(f"Position embeddings shape: {position_embeddings.shape}")

        # åˆå¹¶embeddings
        embeddings = token_embeddings + position_embeddings
        print(f"Combined embeddings shape: {embeddings.shape}")

    # è½¬æ¢ä¸ºnumpyç”¨äºå¯è§†åŒ–
    token_embeddings_np = token_embeddings[0].cpu().numpy()  # [seq_len, hidden_size]
    position_embeddings_np = position_embeddings[0].cpu().numpy()
    embeddings_np = embeddings[0].cpu().numpy()

    valid_length = len(token_labels)

    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Step 2: Query Vector Generation Analysis', fontsize=16, fontweight='bold')

    # 2.1 Token Embeddingsçƒ­åŠ›å›¾
    axes[0, 0].set_title('2.1 Token Embeddings (First 64 dims)', fontsize=12, fontweight='bold')
    im1 = axes[0, 0].imshow(token_embeddings_np[:valid_length, :64].T, cmap='RdBu', aspect='auto')
    axes[0, 0].set_xlabel('Token Position')
    axes[0, 0].set_ylabel('Embedding Dimension')
    axes[0, 0].set_xticks(range(valid_length))
    axes[0, 0].set_xticklabels([label.replace('[', '').replace(']', '') for label in token_labels], rotation=45)
    plt.colorbar(im1, ax=axes[0, 0], shrink=0.8)

    # 2.2 Position Embeddingsçƒ­åŠ›å›¾
    axes[0, 1].set_title('2.2 Position Embeddings (First 64 dims)', fontsize=12, fontweight='bold')
    im2 = axes[0, 1].imshow(position_embeddings_np[:valid_length, :64].T, cmap='RdBu', aspect='auto')
    axes[0, 1].set_xlabel('Token Position')
    axes[0, 1].set_ylabel('Embedding Dimension')
    axes[0, 1].set_xticks(range(valid_length))
    axes[0, 1].set_xticklabels([f'Pos{i}' for i in range(valid_length)], rotation=45)
    plt.colorbar(im2, ax=axes[0, 1], shrink=0.8)

    # 2.3 Combined Embeddingsçƒ­åŠ›å›¾
    axes[1, 0].set_title('2.3 Combined Embeddings (Token + Position)', fontsize=12, fontweight='bold')
    im3 = axes[1, 0].imshow(embeddings_np[:valid_length, :64].T, cmap='RdBu', aspect='auto')
    axes[1, 0].set_xlabel('Token Position')
    axes[1, 0].set_ylabel('Embedding Dimension')
    axes[1, 0].set_xticks(range(valid_length))
    axes[1, 0].set_xticklabels([label.replace('[', '').replace(']', '') for label in token_labels], rotation=45)
    plt.colorbar(im3, ax=axes[1, 0], shrink=0.8)

    # 2.4 å‘é‡ç›¸ä¼¼åº¦åˆ†æ
    axes[1, 1].set_title('2.4 Token Embedding Similarities', fontsize=12, fontweight='bold')

    # è®¡ç®—token embeddingsä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
    token_emb_norm = F.normalize(torch.from_numpy(token_embeddings_np[:valid_length]), dim=1)
    similarity_matrix = torch.mm(token_emb_norm, token_emb_norm.t()).numpy()

    im4 = axes[1, 1].imshow(similarity_matrix, cmap='Blues', vmin=0, vmax=1)
    axes[1, 1].set_xlabel('Token')
    axes[1, 1].set_ylabel('Token')
    axes[1, 1].set_xticks(range(valid_length))
    axes[1, 1].set_yticks(range(valid_length))
    axes[1, 1].set_xticklabels([label.replace('[', '').replace(']', '') for label in token_labels], rotation=45)
    axes[1, 1].set_yticklabels([label.replace('[', '').replace(']', '') for label in token_labels])
    plt.colorbar(im4, ax=axes[1, 1], shrink=0.8)

    # æ·»åŠ ç›¸ä¼¼åº¦æ•°å€¼
    for i in range(valid_length):
        for j in range(valid_length):
            if i != j:  # ä¸æ˜¾ç¤ºå¯¹è§’çº¿
                axes[1, 1].text(j, i, f'{similarity_matrix[i, j]:.2f}',
                                ha='center', va='center', fontsize=8)

    plt.tight_layout()
    plt.savefig(Path(output_dir) / "step2_query_vectors.png", dpi=300, bbox_inches='tight')
    plt.close()

    print("âœ“ æŸ¥è¯¢å‘é‡ç”Ÿæˆåˆ†æå®Œæˆ")
    return embeddings


def step3_position_encoding_analysis(embeddings, token_labels, output_dir):
    """
    æ­¥éª¤3: ä½ç½®ç¼–ç è¯¦ç»†åˆ†æ
    """
    print("=" * 50)
    print("æ­¥éª¤3: ä½ç½®ç¼–ç è¯¦ç»†åˆ†æ")
    print("=" * 50)

    embeddings_np = embeddings[0].cpu().numpy()
    valid_length = len(token_labels)

    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Step 3: Position Encoding Analysis', fontsize=16, fontweight='bold')

    # 3.1 ä½ç½®ç¼–ç æ¨¡å¼
    axes[0, 0].set_title('3.1 Position Encoding Patterns', fontsize=12, fontweight='bold')

    # æ¨¡æ‹Ÿæ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç çš„æ¨¡å¼
    pos_encoding = np.zeros((valid_length, 64))
    for pos in range(valid_length):
        for i in range(32):
            pos_encoding[pos, 2 * i] = np.sin(pos / (10000 ** (2 * i / 64)))
            pos_encoding[pos, 2 * i + 1] = np.cos(pos / (10000 ** (2 * i / 64)))

    im1 = axes[0, 0].imshow(pos_encoding.T, cmap='RdBu', aspect='auto')
    axes[0, 0].set_xlabel('Position')
    axes[0, 0].set_ylabel('Encoding Dimension')
    axes[0, 0].set_xticks(range(valid_length))
    axes[0, 0].set_xticklabels([f'P{i}' for i in range(valid_length)])
    plt.colorbar(im1, ax=axes[0, 0], shrink=0.8)

    # 3.2 ä¸åŒä½ç½®çš„ç¼–ç å‘é‡
    axes[0, 1].set_title('3.2 Position Vectors (First 32 dims)', fontsize=12, fontweight='bold')

    positions_to_show = [0, 2, 5, 8, valid_length - 1]  # CLS, dogs, cushion, sun, SEP
    colors = ['red', 'blue', 'green', 'orange', 'purple']

    for i, (pos, color) in enumerate(zip(positions_to_show, colors)):
        if pos < valid_length:
            axes[0, 1].plot(pos_encoding[pos, :32], color=color, linewidth=2,
                            label=f'{token_labels[pos]} (pos {pos})', alpha=0.8)

    axes[0, 1].set_xlabel('Encoding Dimension')
    axes[0, 1].set_ylabel('Encoding Value')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # 3.3 ä½ç½®è·ç¦»å¯¹ç›¸ä¼¼åº¦çš„å½±å“
    axes[1, 0].set_title('3.3 Position Distance vs Similarity', fontsize=12, fontweight='bold')

    # è®¡ç®—ä¸åŒä½ç½®é—´çš„ç›¸ä¼¼åº¦
    distances = []
    similarities = []

    for i in range(valid_length):
        for j in range(i + 1, valid_length):
            distance = abs(i - j)
            similarity = np.dot(pos_encoding[i], pos_encoding[j]) / (
                    np.linalg.norm(pos_encoding[i]) * np.linalg.norm(pos_encoding[j]))
            distances.append(distance)
            similarities.append(similarity)

    axes[1, 0].scatter(distances, similarities, alpha=0.6, s=50)
    axes[1, 0].set_xlabel('Position Distance')
    axes[1, 0].set_ylabel('Position Encoding Similarity')
    axes[1, 0].grid(True, alpha=0.3)

    # æ·»åŠ è¶‹åŠ¿çº¿
    z = np.polyfit(distances, similarities, 1)
    p = np.poly1d(z)
    axes[1, 0].plot(distances, p(distances), "r--", alpha=0.8, linewidth=2)

    # 3.4 ä½ç½®ç¼–ç çš„ä½œç”¨è¯´æ˜
    axes[1, 1].axis('off')

    explanation = f"""ä½ç½®ç¼–ç çš„ä½œç”¨æœºåˆ¶ï¼š

1. ç»å¯¹ä½ç½®ä¿¡æ¯ï¼š
   â€¢ æ¯ä¸ªä½ç½®éƒ½æœ‰å”¯ä¸€çš„ç¼–ç å‘é‡
   â€¢ ä½¿ç”¨æ­£å¼¦ä½™å¼¦å‡½æ•°ç”Ÿæˆå‘¨æœŸæ€§æ¨¡å¼
   â€¢ ä¸åŒé¢‘ç‡æ•è·ä¸åŒå°ºåº¦çš„ä½ç½®å…³ç³»

2. ç›¸å¯¹ä½ç½®å…³ç³»ï¼š
   â€¢ ç›¸é‚»ä½ç½®çš„ç¼–ç ç›¸ä¼¼åº¦è¾ƒé«˜
   â€¢ è·ç¦»è¶Šè¿œï¼Œç›¸ä¼¼åº¦è¶Šä½
   â€¢ å¸®åŠ©æ¨¡å‹ç†è§£è¯åºå…³ç³»

3. åœ¨æˆ‘ä»¬çš„å¥å­ä¸­ï¼š
   â€¢ "two" (pos 1) å’Œ "dogs" (pos 2) ä½ç½®ç›¸é‚»
   â€¢ "dogs" å’Œ "cushion" (pos 5) æœ‰ä¸€å®šè·ç¦»
   â€¢ ä½ç½®ç¼–ç å¸®åŠ©æ¨¡å‹ç†è§£è¯­æ³•ç»“æ„

4. ä¸Token Embeddingç»“åˆï¼š
   â€¢ Token embedding: è¯æ±‡è¯­ä¹‰ä¿¡æ¯
   â€¢ Position embedding: ä½ç½®åºåˆ—ä¿¡æ¯
   â€¢ ä¸¤è€…ç›¸åŠ å¾—åˆ°å®Œæ•´çš„è¾“å…¥è¡¨ç¤º"""

    axes[1, 1].text(0.05, 0.95, explanation, transform=axes[1, 1].transAxes,
                    fontsize=11, verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow"))

    plt.tight_layout()
    plt.savefig(Path(output_dir) / "step3_position_encoding.png", dpi=300, bbox_inches='tight')
    plt.close()

    print("âœ“ ä½ç½®ç¼–ç åˆ†æå®Œæˆ")


def step4_transformer_analysis(model, text_inputs, token_labels, output_dir):
    """
    æ­¥éª¤4: Transformerå±‚è¯¦ç»†åˆ†æ
    """
    print("=" * 50)
    print("æ­¥éª¤4: Transformerå±‚è¯¦ç»†åˆ†æ")
    print("=" * 50)

    device = next(model.parameters()).device
    text_inputs_device = {k: v.to(device) for k, v in text_inputs.items()}

    # è·å–æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›
    with torch.no_grad():
        outputs = model.text_model(
            input_ids=text_inputs_device['input_ids'],
            attention_mask=text_inputs_device['attention_mask'],
            output_attentions=True,
            output_hidden_states=True
        )

    attentions = outputs.attentions  # 12å±‚æ³¨æ„åŠ›
    hidden_states = outputs.hidden_states  # 13ä¸ªhidden states (embedding + 12å±‚)

    valid_length = len(token_labels)
    content_start = 1  # è·³è¿‡CLS
    content_end = valid_length - 1  # è·³è¿‡SEP
    content_length = content_end - content_start

    print(f"Transformerå±‚æ•°: {len(attentions)}")
    print(f"æ³¨æ„åŠ›å¤´æ•°: {attentions[0].shape[1]}")
    print(f"å†…å®¹tokenæ•°: {content_length}")

    # åˆ›å»ºå¯è§†åŒ–
    fig = plt.figure(figsize=(20, 16))
    fig.suptitle('Step 4: Transformer Layer Analysis', fontsize=16, fontweight='bold')

    # 4.1 ä¸åŒå±‚çš„æ³¨æ„åŠ›æ¨¡å¼æ¼”å˜
    ax1 = plt.subplot2grid((4, 4), (0, 0), colspan=4)
    ax1.set_title('4.1 Attention Pattern Evolution Across Layers', fontsize=14, fontweight='bold')

    # é€‰æ‹©å‡ ä¸ªå…³é”®å±‚è¿›è¡Œå±•ç¤º
    layers_to_show = [0, 3, 7, 11]  # ç¬¬1ã€4ã€8ã€12å±‚
    layer_names = ['Layer 1', 'Layer 4', 'Layer 8', 'Layer 12']

    for i, (layer_idx, layer_name) in enumerate(zip(layers_to_show, layer_names)):
        ax = plt.subplot2grid((4, 4), (1, i))

        # å¹³å‡æ‰€æœ‰æ³¨æ„åŠ›å¤´ï¼Œåªçœ‹å†…å®¹token
        layer_attention = attentions[layer_idx][0].mean(dim=0)  # å¹³å‡æ‰€æœ‰å¤´
        content_attention = layer_attention[content_start:content_end, content_start:content_end]

        im = ax.imshow(content_attention.cpu().numpy(), cmap='Blues', vmin=0, vmax=1)
        ax.set_title(layer_name, fontsize=12)
        ax.set_xticks(range(content_length))
        ax.set_yticks(range(content_length))
        content_labels = token_labels[content_start:content_end]
        ax.set_xticklabels(content_labels, rotation=45, fontsize=8)
        ax.set_yticklabels(content_labels, fontsize=8)

        if i == len(layers_to_show) - 1:
            plt.colorbar(im, ax=ax, shrink=0.8)

    # 4.2 å¤šå¤´æ³¨æ„åŠ›åˆ†æ
    ax2 = plt.subplot2grid((4, 4), (2, 0), colspan=2)
    ax2.set_title('4.2 Multi-Head Attention Patterns (Layer 12)', fontsize=14, fontweight='bold')

    # åˆ†ææœ€åä¸€å±‚çš„å¤šå¤´æ³¨æ„åŠ›
    final_attention = attentions[-1][0]  # [num_heads, seq_len, seq_len]
    num_heads = min(8, final_attention.shape[0])

    head_patterns = []
    for head in range(num_heads):
        head_att = final_attention[head, content_start:content_end, content_start:content_end]
        diag_strength = torch.diag(head_att).mean().item()
        off_diag_strength = (head_att.sum() - torch.diag(head_att).sum()).item() / (
                    content_length * content_length - content_length)
        head_patterns.append([diag_strength, off_diag_strength])

    head_patterns = np.array(head_patterns)
    x = np.arange(num_heads)
    width = 0.35

    bars1 = ax2.bar(x - width / 2, head_patterns[:, 0], width, label='Self-Attention', alpha=0.8, color='orange')
    bars2 = ax2.bar(x + width / 2, head_patterns[:, 1], width, label='Cross-Attention', alpha=0.8, color='lightblue')

    ax2.set_xlabel('Attention Head')
    ax2.set_ylabel('Attention Strength')
    ax2.set_xticks(x)
    ax2.set_xticklabels([f'H{i + 1}' for i in range(num_heads)])
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 4.3 Hidden Statesæ¼”å˜
    ax3 = plt.subplot2grid((4, 4), (2, 2), colspan=2)
    ax3.set_title('4.3 Hidden States Evolution', fontsize=14, fontweight='bold')

    # è®¡ç®—æ¯å±‚hidden statesçš„å˜åŒ–
    layer_changes = []
    for i in range(1, len(hidden_states)):
        prev_hidden = hidden_states[i - 1][0, content_start:content_end]  # å†…å®¹token
        curr_hidden = hidden_states[i][0, content_start:content_end]

        # è®¡ç®—å˜åŒ–å¹…åº¦ï¼ˆL2èŒƒæ•°ï¼‰
        change = torch.norm(curr_hidden - prev_hidden, dim=1).mean().item()
        layer_changes.append(change)

    ax3.plot(range(1, len(hidden_states)), layer_changes, 'o-', linewidth=2, markersize=6)
    ax3.set_xlabel('Layer')
    ax3.set_ylabel('Hidden State Change')
    ax3.grid(True, alpha=0.3)
    ax3.set_title('Hidden State Changes Between Layers')

    # 4.4 å…³é”®è¯"dogs"çš„æ³¨æ„åŠ›æ¼”å˜
    ax4 = plt.subplot2grid((4, 4), (3, 0), colspan=4)
    ax4.set_title('4.4 "dogs" Token Attention Evolution Across Layers', fontsize=14, fontweight='bold')

    dogs_idx = 1  # "dogs"åœ¨å†…å®¹tokenä¸­çš„ä½ç½® (0:two, 1:dogs, ...)
    dogs_attention_evolution = []

    for layer_idx in range(len(attentions)):
        layer_attention = attentions[layer_idx][0].mean(dim=0)  # å¹³å‡æ‰€æœ‰å¤´
        dogs_attention = layer_attention[content_start + dogs_idx, content_start:content_end]
        dogs_attention_evolution.append(dogs_attention.cpu().numpy())

    dogs_attention_evolution = np.array(dogs_attention_evolution)

    im4 = ax4.imshow(dogs_attention_evolution.T, cmap='Blues', aspect='auto')
    ax4.set_xlabel('Layer')
    ax4.set_ylabel('Target Token')
    ax4.set_xticks(range(len(attentions)))
    ax4.set_xticklabels([f'L{i + 1}' for i in range(len(attentions))])
    ax4.set_yticks(range(content_length))
    ax4.set_yticklabels(content_labels)
    plt.colorbar(im4, ax=ax4, shrink=0.8)

    plt.tight_layout()
    plt.savefig(Path(output_dir) / "step4_transformer_analysis.png", dpi=300, bbox_inches='tight')
    plt.close()

    print("âœ“ Transformerå±‚åˆ†æå®Œæˆ")
    return outputs


def step5_sentence_vector_analysis(model, text_inputs, token_labels, outputs, output_dir):
    """
    æ­¥éª¤5: å¥å­è¯­ä¹‰å‘é‡ç”Ÿæˆåˆ†æ
    """
    print("=" * 50)
    print("æ­¥éª¤5: å¥å­è¯­ä¹‰å‘é‡ç”Ÿæˆåˆ†æ")
    print("=" * 50)

    device = next(model.parameters()).device
    text_inputs_device = {k: v.to(device) for k, v in text_inputs.items()}

    # è·å–æœ€ç»ˆçš„å¥å­è¡¨ç¤º
    with torch.no_grad():
        # è·å–CLS tokençš„è¡¨ç¤ºï¼ˆå¥å­çº§åˆ«è¡¨ç¤ºï¼‰
        cls_hidden = outputs.last_hidden_state[0, 0]  # CLS token

        # é€šè¿‡projection headå¾—åˆ°æœ€ç»ˆçš„æ–‡æœ¬embedding
        text_embeds = model.text_projection(cls_hidden)

        # å½’ä¸€åŒ–
        text_embeds_norm = F.normalize(text_embeds, dim=0)

        print(f"CLS hidden state shape: {cls_hidden.shape}")
        print(f"Text embedding shape: {text_embeds.shape}")
        print(f"Text embedding norm: {torch.norm(text_embeds_norm).item():.4f}")

    # åˆ›å»ºå¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Step 5: Sentence Semantic Vector Analysis', fontsize=16, fontweight='bold')

    # 5.1 CLS tokenæ¼”å˜è¿‡ç¨‹
    axes[0, 0].set_title('5.1 CLS Token Evolution Across Layers', fontsize=12, fontweight='bold')

    # æ”¶é›†æ‰€æœ‰å±‚çš„CLS tokenè¡¨ç¤º
    cls_evolution = []
    for hidden_state in outputs.hidden_states:
        cls_token = hidden_state[0, 0].cpu().numpy()  # CLS token
        cls_evolution.append(cls_token)

    cls_evolution = np.array(cls_evolution)  # [num_layers+1, hidden_size]

    # æ˜¾ç¤ºå‰64ç»´çš„æ¼”å˜
    im1 = axes[0, 0].imshow(cls_evolution[:, :64].T, cmap='RdBu', aspect='auto')
    axes[0, 0].set_xlabel('Layer')
    axes[0, 0].set_ylabel('Hidden Dimension')
    axes[0, 0].set_xticks(range(len(cls_evolution)))
    axes[0, 0].set_xticklabels(['Embed'] + [f'L{i + 1}' for i in range(len(cls_evolution) - 1)])
    plt.colorbar(im1, ax=axes[0, 0], shrink=0.8)

    # 5.2 CLS tokenä¸å…¶ä»–tokençš„ç›¸ä¼¼åº¦
    axes[0, 1].set_title('5.2 CLS Token Similarity with Content Tokens', fontsize=12, fontweight='bold')

    final_hidden = outputs.last_hidden_state[0].cpu()  # [seq_len, hidden_size]
    cls_final = final_hidden[0]  # CLS token
    content_tokens = final_hidden[1:-1]  # å†…å®¹tokensï¼ˆå»æ‰CLSå’ŒSEPï¼‰

    # è®¡ç®—ç›¸ä¼¼åº¦
    cls_norm = F.normalize(cls_final.unsqueeze(0), dim=1)
    content_norm = F.normalize(content_tokens, dim=1)
    similarities = torch.mm(cls_norm, content_norm.t())[0].numpy()

    content_labels = token_labels[1:-1]  # å»æ‰CLSå’ŒSEP
    bars = axes[0, 1].bar(range(len(similarities)), similarities,
                          color='lightcoral', alpha=0.7)
    axes[0, 1].set_xlabel('Content Token')
    axes[0, 1].set_ylabel('Similarity with CLS')
    axes[0, 1].set_xticks(range(len(similarities)))
    axes[0, 1].set_xticklabels(content_labels, rotation=45)
    axes[0, 1].grid(True, alpha=0.3)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, sim in zip(bars, similarities):
        height = bar.get_height()
        axes[0, 1].text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                        f'{sim:.3f}', ha='center', va='bottom', fontsize=9)

    # 5.3 æœ€ç»ˆæ–‡æœ¬embeddingçš„ç‰¹å¾åˆ†å¸ƒ
    axes[1, 0].set_title('5.3 Final Text Embedding Distribution', fontsize=12, fontweight='bold')

    text_embeds_np = text_embeds_norm.cpu().numpy()

    # ç»˜åˆ¶embeddingå€¼çš„åˆ†å¸ƒ
    axes[1, 0].hist(text_embeds_np, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    axes[1, 0].set_xlabel('Embedding Value')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].grid(True, alpha=0.3)

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    mean_val = np.mean(text_embeds_np)
    std_val = np.std(text_embeds_np)
    axes[1, 0].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.3f}')
    axes[1, 0].axvline(mean_val + std_val, color='orange', linestyle='--', alpha=0.7, label=f'Â±Std: {std_val:.3f}')
    axes[1, 0].axvline(mean_val - std_val, color='orange', linestyle='--', alpha=0.7)
    axes[1, 0].legend()

    # 5.4 å¥å­è¯­ä¹‰å‘é‡ç”Ÿæˆè¿‡ç¨‹æ€»ç»“
    axes[1, 1].axis('off')

    # è®¡ç®—ä¸€äº›å…³é”®ç»Ÿè®¡ä¿¡æ¯
    max_sim_idx = np.argmax(similarities)
    max_sim_token = content_labels[max_sim_idx]
    max_sim_value = similarities[max_sim_idx]

    explanation = f"""å¥å­è¯­ä¹‰å‘é‡ç”Ÿæˆè¿‡ç¨‹ï¼š

1. CLS Tokençš„ä½œç”¨ï¼š
   â€¢ åˆå§‹åŒ–ä¸ºç‰¹æ®Šçš„[CLS] embedding
   â€¢ é€šè¿‡12å±‚Transformeré€æ­¥èšåˆä¿¡æ¯
   â€¢ æœ€ç»ˆæˆä¸ºæ•´ä¸ªå¥å­çš„è¯­ä¹‰è¡¨ç¤º

2. ä¿¡æ¯èšåˆæœºåˆ¶ï¼š
   â€¢ CLS tokené€šè¿‡æ³¨æ„åŠ›æœºåˆ¶å…³æ³¨æ‰€æœ‰å†…å®¹token
   â€¢ æœ€ç›¸ä¼¼çš„å†…å®¹token: "{max_sim_token}" ({max_sim_value:.3f})
   â€¢ é€å±‚èåˆè¯­æ³•å’Œè¯­ä¹‰ä¿¡æ¯

3. æœ€ç»ˆè¡¨ç¤ºç‰¹å¾ï¼š
   â€¢ ç»´åº¦: {text_embeds.shape[0]}
   â€¢ L2èŒƒæ•°: {torch.norm(text_embeds_norm).item():.4f} (å·²å½’ä¸€åŒ–)
   â€¢ å‡å€¼: {mean_val:.3f}, æ ‡å‡†å·®: {std_val:.3f}

4. è¯­ä¹‰ç¼–ç ç»“æœï¼š
   â€¢ æ•è·äº†"ä¸¤åªç‹—åœ¨å«å­ä¸Šæ™’å¤ªé˜³"çš„å®Œæ•´è¯­ä¹‰
   â€¢ å¯ç”¨äºä¸å›¾åƒç‰¹å¾è¿›è¡Œè·¨æ¨¡æ€åŒ¹é…
   â€¢ åŒ…å«äº†è¯æ±‡ã€è¯­æ³•ã€è¯­ä¹‰çš„ç»¼åˆä¿¡æ¯

5. ä»Tokenåˆ°å¥å­çš„è½¬æ¢ï¼š
   â€¢ Token level: å•è¯çº§åˆ«çš„è¯­ä¹‰
   â€¢ Sentence level: å¥å­çº§åˆ«çš„è¯­ä¹‰
   â€¢ é€šè¿‡CLS tokenå®ç°ä¿¡æ¯çš„å…¨å±€æ•´åˆ"""

    axes[1, 1].text(0.05, 0.95, explanation, transform=axes[1, 1].transAxes,
                    fontsize=11, verticalalignment='top', fontfamily='monospace',
                    bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow"))

    plt.tight_layout()
    plt.savefig(Path(output_dir) / "step5_sentence_vector.png", dpi=300, bbox_inches='tight')
    plt.close()

    print("âœ“ å¥å­è¯­ä¹‰å‘é‡åˆ†æå®Œæˆ")
    return text_embeds_norm


def create_process_summary(output_dir):
    """
    åˆ›å»ºæ•´ä¸ªè¿‡ç¨‹çš„æ€»ç»“å›¾
    """
    print("=" * 50)
    print("åˆ›å»ºè¿‡ç¨‹æ€»ç»“")
    print("=" * 50)

    fig, ax = plt.subplots(1, 1, figsize=(16, 10))
    fig.suptitle('Complete Process: From Tokenization to Sentence Vector\n"two dogs lying on a cushion in the sun"',
                 fontsize=16, fontweight='bold')

    ax.axis('off')

    # ç»˜åˆ¶æµç¨‹å›¾
    steps = [
        "1. Tokenization\nåˆ†è¯",
        "2. Query Vectors\næŸ¥è¯¢å‘é‡",
        "3. Position Encoding\nä½ç½®ç¼–ç ",
        "4. Transformer\nTransformerå±‚",
        "5. Sentence Vector\nå¥å­è¯­ä¹‰å‘é‡"
    ]

    descriptions = [
        "â€¢ æ–‡æœ¬ â†’ Tokenåºåˆ—\nâ€¢ 11ä¸ªtokens\nâ€¢ [CLS] + 9å†…å®¹ + [SEP]",
        "â€¢ Token â†’ Embedding\nâ€¢ 512ç»´å‘é‡\nâ€¢ è¯æ±‡ + ä½ç½®ä¿¡æ¯",
        "â€¢ ç»å¯¹ä½ç½®ä¿¡æ¯\nâ€¢ ç›¸å¯¹ä½ç½®å…³ç³»\nâ€¢ æ­£å¼¦ä½™å¼¦ç¼–ç ",
        "â€¢ 12å±‚æ³¨æ„åŠ›æœºåˆ¶\nâ€¢ 8ä¸ªæ³¨æ„åŠ›å¤´\nâ€¢ è‡ªæ³¨æ„åŠ› + è·¨è¯æ³¨æ„åŠ›",
        "â€¢ CLS tokenè¡¨ç¤º\nâ€¢ 512ç»´è¯­ä¹‰å‘é‡\nâ€¢ å¥å­çº§åˆ«è¯­ä¹‰"
    ]

    # ç»˜åˆ¶æ­¥éª¤æ¡†
    box_width = 0.15
    box_height = 0.3
    y_center = 0.6

    for i, (step, desc) in enumerate(zip(steps, descriptions)):
        x_center = 0.1 + i * 0.2

        # ç»˜åˆ¶æ­¥éª¤æ¡†
        rect = plt.Rectangle((x_center - box_width / 2, y_center - box_height / 2),
                             box_width, box_height,
                             facecolor='lightblue', edgecolor='black', linewidth=2)
        ax.add_patch(rect)

        # æ·»åŠ æ­¥éª¤æ ‡é¢˜
        ax.text(x_center, y_center + 0.05, step, ha='center', va='center',
                fontsize=12, fontweight='bold')

        # æ·»åŠ æè¿°
        ax.text(x_center, y_center - 0.05, desc, ha='center', va='center',
                fontsize=10, style='italic')

        # ç»˜åˆ¶ç®­å¤´ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
        if i < len(steps) - 1:
            ax.annotate('', xy=(x_center + box_width / 2 + 0.02, y_center),
                        xytext=(x_center + box_width / 2, y_center),
                        arrowprops=dict(arrowstyle='->', lw=2, color='red'))

    # æ·»åŠ è¯¦ç»†è¯´æ˜
    summary_text = """å®Œæ•´çš„æ–‡æœ¬ç†è§£è¿‡ç¨‹ï¼š

è¾“å…¥: "two dogs lying on a cushion in the sun"

æ­¥éª¤1 - åˆ†è¯ (Tokenization):
â€¢ å°†åŸå§‹æ–‡æœ¬åˆ†è§£ä¸ºå¯å¤„ç†çš„tokenå•å…ƒ
â€¢ æ·»åŠ ç‰¹æ®Šæ ‡è®°[CLS]å’Œ[SEP]ç”¨äºå¥å­çº§åˆ«å¤„ç†
â€¢ æ¯ä¸ªtokenæ˜ å°„åˆ°å”¯ä¸€çš„ID

æ­¥éª¤2 - æŸ¥è¯¢å‘é‡ (Query Vectors):
â€¢ å°†token IDè½¬æ¢ä¸ºé«˜ç»´embeddingå‘é‡
â€¢ Token embeddingåŒ…å«è¯æ±‡è¯­ä¹‰ä¿¡æ¯
â€¢ Position embeddingåŒ…å«ä½ç½®åºåˆ—ä¿¡æ¯

æ­¥éª¤3 - ä½ç½®ç¼–ç  (Position Encoding):
â€¢ ä¸ºæ¯ä¸ªä½ç½®ç”Ÿæˆå”¯ä¸€çš„ç¼–ç å‘é‡
â€¢ ä½¿ç”¨æ­£å¼¦ä½™å¼¦å‡½æ•°æ•è·ä½ç½®å…³ç³»
â€¢ å¸®åŠ©æ¨¡å‹ç†è§£è¯åºå’Œè¯­æ³•ç»“æ„

æ­¥éª¤4 - Transformerå¤„ç†:
â€¢ 12å±‚è‡ªæ³¨æ„åŠ›æœºåˆ¶é€æ­¥å¤„ç†ä¿¡æ¯
â€¢ æ¯å±‚8ä¸ªæ³¨æ„åŠ›å¤´æ•è·ä¸åŒç±»å‹çš„å…³ç³»
â€¢ è¯æ±‡é—´çš„ä¾èµ–å…³ç³»é€æ­¥å»ºç«‹å’Œå¼ºåŒ–

æ­¥éª¤5 - å¥å­è¯­ä¹‰å‘é‡:
â€¢ CLS tokenèšåˆæ‰€æœ‰è¯æ±‡ä¿¡æ¯
â€¢ ç”Ÿæˆå›ºå®šé•¿åº¦çš„å¥å­çº§åˆ«è¡¨ç¤º
â€¢ å¯ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚å›¾æ–‡åŒ¹é…ï¼‰

æœ€ç»ˆç»“æœ: 512ç»´çš„å¥å­è¯­ä¹‰å‘é‡ï¼ŒåŒ…å«äº†å®Œæ•´çš„è¯­ä¹‰ä¿¡æ¯"""

    ax.text(0.5, 0.25, summary_text, ha='center', va='top',
            fontsize=11, fontfamily='monospace',
            bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))

    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)

    plt.tight_layout()
    plt.savefig(Path(output_dir) / "step6_process_summary.png", dpi=300, bbox_inches='tight')
    plt.close()

    print("âœ“ è¿‡ç¨‹æ€»ç»“åˆ›å»ºå®Œæˆ")


def run_detailed_attention_analysis_v2():
    """
    è¿è¡Œè¯¦ç»†çš„æ³¨æ„åŠ›åˆ†æ v2.0
    æŒ‰ç…§ï¼šåˆ†è¯ â†’ æŸ¥è¯¢å‘é‡ â†’ ä½ç½®ç¼–ç  â†’ transformer â†’ å¥å­è¯­ä¹‰å‘é‡ çš„è·¯å¾„
    """
    print("ğŸš€ å¯åŠ¨è‹±æ–‡æ–‡æœ¬æ³¨æ„åŠ›æœºåˆ¶è¿‡ç¨‹åˆ†æ v2.0")
    print("ğŸ“Š åˆ†æè·¯å¾„ï¼šåˆ†è¯ â†’ æŸ¥è¯¢å‘é‡ â†’ ä½ç½®ç¼–ç  â†’ transformer â†’ å¥å­è¯­ä¹‰å‘é‡")
    print("ğŸ¯ ç›®æ ‡æ–‡æœ¬ï¼š'two dogs lying on a cushion in the sun'")
    print("=" * 80)

    # è®¾ç½®è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. åŠ è½½æ¨¡å‹
    print("åŠ è½½CLIPæ¨¡å‹...")
    model, tokenizer, model_loaded = load_local_clip_model()
    if not model_loaded:
        print("âŒ æ— æ³•åŠ è½½æ¨¡å‹")
        return

    model = model.to(device).eval()
    print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")

    # 2. å‡†å¤‡æ–‡æœ¬
    target_text = "two dogs lying on a cushion in the sun"
    print(f"âœ“ ç›®æ ‡æ–‡æœ¬: {target_text}")

    # 3. åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(__file__).parent / "outputs" / "detailed_attention_v2"
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ“ è¾“å‡ºç›®å½•: {output_dir}")

    # æ‰§è¡Œåˆ†ææ­¥éª¤
    try:
        # æ­¥éª¤1: åˆ†è¯åˆ†æ
        text_inputs, token_labels = step1_tokenization_analysis(tokenizer, target_text, output_dir)

        # æ­¥éª¤2: æŸ¥è¯¢å‘é‡åˆ†æ
        embeddings = step2_query_vector_analysis(model, text_inputs, token_labels, output_dir)

        # æ­¥éª¤3: ä½ç½®ç¼–ç åˆ†æ
        step3_position_encoding_analysis(embeddings, token_labels, output_dir)

        # æ­¥éª¤4: Transformeråˆ†æ
        outputs = step4_transformer_analysis(model, text_inputs, token_labels, output_dir)

        # æ­¥éª¤5: å¥å­è¯­ä¹‰å‘é‡åˆ†æ
        sentence_vector = step5_sentence_vector_analysis(model, text_inputs, token_labels, outputs, output_dir)

        # æ­¥éª¤6: è¿‡ç¨‹æ€»ç»“
        create_process_summary(output_dir)

        print("=" * 80)
        print("âœ… è¯¦ç»†æ³¨æ„åŠ›åˆ†æ v2.0 å®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")
        print("ğŸ“„ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   1. step1_tokenization.png - åˆ†è¯è¿‡ç¨‹åˆ†æ")
        print("   2. step2_query_vectors.png - æŸ¥è¯¢å‘é‡ç”Ÿæˆ")
        print("   3. step3_position_encoding.png - ä½ç½®ç¼–ç åˆ†æ")
        print("   4. step4_transformer_analysis.png - Transformerå±‚åˆ†æ")
        print("   5. step5_sentence_vector.png - å¥å­è¯­ä¹‰å‘é‡")
        print("   6. step6_process_summary.png - å®Œæ•´è¿‡ç¨‹æ€»ç»“")
        print("=" * 80)

        # å°è¯•æ‰“å¼€æ–‡ä»¶å¤¹ï¼ˆmacOSï¼‰
        import subprocess
        import sys
        try:
            if sys.platform == "darwin":  # macOS
                subprocess.run(["open", str(output_dir)], check=False)
                print("ğŸ“‚ æ–‡ä»¶å¤¹å·²è‡ªåŠ¨æ‰“å¼€")
        except:
            pass

        return output_dir

    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œè¯¦ç»†æ¼”ç¤º
    run_detailed_attention_analysis_v2();
