import os

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# å¿½ç•¥NumPyè­¦å‘Š
import warnings

warnings.filterwarnings('ignore', category=FutureWarning)

import torch
import torch.nn.functional as F
from transformers import AutoModel, AutoProcessor, CLIPModel, CLIPProcessor
from PIL import Image, ImageDraw
import numpy as np
from pathlib import Path

print(f"ğŸ” æ£€æŸ¥transformersç‰ˆæœ¬...")
import transformers

print(f"transformersç‰ˆæœ¬: {transformers.__version__}")


class VideoTextModels:
    """åŸºäºCLIPçš„è§†é¢‘-æ–‡å­—å¤„ç†SOTAæ¨¡å‹é›†åˆ"""

    def __init__(self):
        self.models = {}
        self.processors = {}

    def load_clip_video1_videoclip(self):
        """æ¨¡å‹1: VideoCLIP - åŸºäºCLIPçš„è§†é¢‘ç†è§£æ¨¡å‹"""
        print("ğŸ”„ åŠ è½½ VideoCLIP æ¨¡å‹...")
        try:
            model_name = "openai/clip-vit-base-patch32"

            # ä½¿ç”¨CLIPModelå’ŒCLIPProcessorï¼ˆæ›´ç¨³å®šï¼‰
            self.models['clip_video1'] = CLIPModel.from_pretrained(
                model_name,
                cache_dir="./models/clip_video1"
            )

            self.processors['clip_video1'] = CLIPProcessor.from_pretrained(
                model_name,
                cache_dir="./models/clip_video1"
            )

            print("âœ… VideoCLIP (åŸºç¡€ç‰ˆ) åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ VideoCLIP åŠ è½½å¤±è´¥: {e}")
            return False

    def load_clip_video2_xclip(self):
        """æ¨¡å‹2: X-CLIP - å¤§æ¨¡å‹ç‰ˆæœ¬"""
        print("ğŸ”„ åŠ è½½ X-CLIP æ¨¡å‹...")
        try:
            model_name = "openai/clip-vit-large-patch14"

            self.models['clip_video2'] = CLIPModel.from_pretrained(
                model_name,
                cache_dir="./models/clip_video2"
            )

            self.processors['clip_video2'] = CLIPProcessor.from_pretrained(
                model_name,
                cache_dir="./models/clip_video2"
            )
            print("âœ… X-CLIP (å¤§æ¨¡å‹ç‰ˆ) åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ X-CLIP åŠ è½½å¤±è´¥: {e}")
            return False

    def load_clip_video3_patch16(self):
        """æ¨¡å‹3: CLIP-Patch16 - æ›´ç»†ç²’åº¦çš„patchç‰ˆæœ¬"""
        print("ğŸ”„ åŠ è½½ CLIP-Patch16 æ¨¡å‹...")
        try:
            model_name = "openai/clip-vit-base-patch16"

            self.models['clip_video3'] = CLIPModel.from_pretrained(
                model_name,
                cache_dir="./models/clip_video3"
            )

            self.processors['clip_video3'] = CLIPProcessor.from_pretrained(
                model_name,
                cache_dir="./models/clip_video3"
            )
            print("âœ… CLIP-Patch16 åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ CLIP-Patch16 åŠ è½½å¤±è´¥: {e}")
            return False

    def load_all_models(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹åŠ è½½æ‰€æœ‰è§†é¢‘-æ–‡å­—å¤„ç†æ¨¡å‹...")
        print("=" * 80)

        models_info = [
            ("VideoCLIP (Base-Patch32)", self.load_clip_video1_videoclip),
            ("X-CLIP (Large-Patch14)", self.load_clip_video2_xclip),
            ("CLIP (Base-Patch16)", self.load_clip_video3_patch16),
        ]

        success_count = 0
        for name, load_func in models_info:
            print(f"\nğŸ“¦ æ­£åœ¨åŠ è½½ {name}...")
            if load_func():
                success_count += 1
            print("-" * 40)

        print(f"\nâœ… æˆåŠŸåŠ è½½ {success_count}/{len(models_info)} ä¸ªæ¨¡å‹")
        return success_count > 0


class VideoProcessor:
    """è§†é¢‘å¤„ç†å·¥å…·ç±»"""

    @staticmethod
    def create_sample_images(output_dir="./test_materials", num_images=8):
        """åˆ›å»ºç¤ºä¾‹å›¾åƒåºåˆ—æ¨¡æ‹Ÿè§†é¢‘å¸§"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        images = []

        for i in range(num_images):
            # åˆ›å»º640x480çš„å›¾åƒ
            img = Image.new('RGB', (640, 480), color=(50, 50, 50))
            draw = ImageDraw.Draw(img)

            # æ·»åŠ ç§»åŠ¨çš„åœ†å½¢
            center_x = int(320 + 200 * np.sin(2 * np.pi * i / num_images))
            center_y = int(240 + 100 * np.cos(2 * np.pi * i / num_images))

            # ç»˜åˆ¶åœ†å½¢
            radius = 50
            draw.ellipse([
                center_x - radius, center_y - radius,
                center_x + radius, center_y + radius
            ], fill=(255, 255, 0))

            # æ·»åŠ æ–‡å­—
            draw.text((50, 50), f"Frame {i + 1}/{num_images}",
                      fill=(255, 255, 255))

            # è°ƒæ•´å¤§å°åˆ°224x224ï¼ˆCLIPæ ‡å‡†è¾“å…¥å°ºå¯¸ï¼‰
            img_resized = img.resize((224, 224))
            images.append(img_resized)

            # ä¿å­˜å›¾åƒæ–‡ä»¶
            img_path = output_path / f"frame_{i:03d}.jpg"
            img_resized.save(img_path)

        print(f"âœ… åˆ›å»ºäº† {num_images} ä¸ªç¤ºä¾‹å›¾åƒå¸§")
        return images


class VideoTextDemo:
    """è§†é¢‘-æ–‡å­—å¤„ç†æ¼”ç¤ºç±»"""

    def __init__(self):
        self.video_models = VideoTextModels()
        self.video_processor = VideoProcessor()

    def setup_test_materials(self):
        """å‡†å¤‡æµ‹è¯•ç´ æ"""
        print("ğŸ¬ å‡†å¤‡æµ‹è¯•ç´ æ...")

        # åˆ›å»ºæµ‹è¯•ç›®å½•
        test_dir = Path("./test_materials")
        test_dir.mkdir(exist_ok=True)

        # 1. åˆ›å»ºç¤ºä¾‹å›¾åƒåºåˆ—ï¼ˆæ¨¡æ‹Ÿè§†é¢‘å¸§ï¼‰
        frames = self.video_processor.create_sample_images(str(test_dir))

        # 2. å‡†å¤‡æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "a person walking in the park",
            "a moving colorful circle",
            "animated graphics with text",
            "a ball bouncing around",
            "computer generated animation"
        ]

        # ä¿å­˜æµ‹è¯•æ–‡æœ¬
        text_file = test_dir / "test_texts.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            for text in test_texts:
                f.write(text + '\n')

        print(f"âœ… æµ‹è¯•ç´ æå‡†å¤‡å®Œæˆ:")
        print(f"   - å›¾åƒå¸§: {len(frames)} ä¸ª")
        print(f"   - æ–‡æœ¬æ–‡ä»¶: {text_file}")

        return frames, test_texts

    def demonstrate_video_text_matching(self, model_name="clip_video1"):
        """æ¼”ç¤ºè§†é¢‘-æ–‡æœ¬åŒ¹é…"""
        print(f"\nğŸ¯ ä½¿ç”¨ {model_name} è¿›è¡Œè§†é¢‘-æ–‡æœ¬åŒ¹é…æ¼”ç¤º")
        print("=" * 60)

        # å‡†å¤‡æµ‹è¯•ç´ æ
        frames, test_texts = self.setup_test_materials()

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
        if model_name not in self.video_models.models:
            print(f"âŒ æ¨¡å‹ {model_name} æœªåŠ è½½")
            return

        model = self.video_models.models[model_name]
        processor = self.video_models.processors[model_name]

        print("ğŸï¸ å¤„ç†è§†é¢‘å¸§...")
        print(f"   ä½¿ç”¨ {len(frames)} å¸§è¿›è¡Œåˆ†æ")

        # å¤„ç†è¾“å…¥
        print("ğŸ”„ å¤„ç†è¾“å…¥æ•°æ®...")
        try:
            # ä½¿ç”¨ç¬¬ä¸€å¸§ä½œä¸ºä»£è¡¨å›¾åƒ
            representative_frame = frames[0]

            inputs = processor(
                images=representative_frame,
                text=test_texts,
                return_tensors="pt",
                padding=True
            )

            print("âœ… è¾“å…¥æ•°æ®å¤„ç†å®Œæˆ")
            print(f"   å›¾åƒtensor shape: {inputs['pixel_values'].shape}")
            print(f"   æ–‡æœ¬tokens shape: {inputs['input_ids'].shape}")

            # æ¨¡å‹æ¨ç†
            print("ğŸ§  è¿›è¡Œæ¨¡å‹æ¨ç†...")
            with torch.no_grad():
                outputs = model(**inputs)

                # è·å–ç›¸ä¼¼åº¦
                logits_per_image = outputs.logits_per_image
                probs = F.softmax(logits_per_image, dim=-1)

                print("\nğŸ“Š è§†é¢‘-æ–‡æœ¬åŒ¹é…ç»“æœ:")
                for i, text in enumerate(test_texts):
                    score = probs[0][i].item()
                    print(f"   '{text}': {score:.4f}")

        except Exception as e:
            print(f"âŒ æ¨ç†è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    def run_comprehensive_demo(self):
        """è¿è¡Œå®Œæ•´çš„æ¼”ç¤º"""
        print("ğŸ¬ è§†é¢‘-æ–‡å­—å¤„ç†æ¨¡å‹ç»¼åˆæ¼”ç¤º")
        print("=" * 80)

        # åŠ è½½æ¨¡å‹
        if not self.video_models.load_all_models():
            print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ¨¡å‹ï¼Œé€€å‡ºæ¼”ç¤º")
            return

        # å¯¹æ¯ä¸ªæˆåŠŸåŠ è½½çš„æ¨¡å‹è¿›è¡Œæ¼”ç¤º
        for model_name in self.video_models.models.keys():
            self.demonstrate_video_text_matching(model_name)
            print("\n" + "=" * 60 + "\n")


def show_model_papers():
    """æ˜¾ç¤ºæ¨èçš„æ¨¡å‹å’Œè®ºæ–‡"""
    print("\nğŸ“š æ¨èçš„è§†é¢‘-æ–‡å­—å¤„ç†SOTAæ¨¡å‹:")
    print("=" * 80)

    papers = [
        {
            "name": "CLIP",
            "paper": "Learning Transferable Visual Models From Natural Language Supervision",
            "url": "https://arxiv.org/abs/2103.00020",
            "description": "OpenAIçš„åŸå§‹CLIPæ¨¡å‹ï¼Œå¥ å®šäº†è§†è§‰-è¯­è¨€ç†è§£åŸºç¡€"
        },
        {
            "name": "VideoCLIP",
            "paper": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding",
            "url": "https://arxiv.org/abs/2109.14084",
            "description": "å°†CLIPæ‰©å±•åˆ°è§†é¢‘ç†è§£é¢†åŸŸ"
        },
        {
            "name": "X-CLIP",
            "paper": "Expanding Language-Image Pretrained Models for General Video Recognition",
            "url": "https://arxiv.org/abs/2208.02816",
            "description": "å¾®è½¯æå‡ºçš„è·¨æ¨¡æ€è§†é¢‘ç†è§£æ¨¡å‹"
        },
        {
            "name": "VideoMAE",
            "paper": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
            "url": "https://arxiv.org/abs/2203.12602",
            "description": "åŸºäºæ©ç è‡ªç¼–ç å™¨çš„è§†é¢‘ç†è§£"
        }
    ]

    for i, paper in enumerate(papers, 1):
        print(f"\n{i}. **{paper['name']}**")
        print(f"   ğŸ“„ è®ºæ–‡: {paper['paper']}")
        print(f"   ğŸ”— é“¾æ¥: {paper['url']}")
        print(f"   ğŸ’¡ æè¿°: {paper['description']}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ åŸºäºCLIPçš„è§†é¢‘-æ–‡å­—å¤„ç†SOTAæ¨¡å‹æ¼”ç¤º")
    print("ğŸ”§ ä½¿ç”¨å‡çº§åçš„transformersåº“")
    print("=" * 80)

    # æ˜¾ç¤ºæ¨èæ¨¡å‹å’Œè®ºæ–‡
    show_model_papers()

    try:
        # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
        demo = VideoTextDemo()

        # è¿è¡Œç»¼åˆæ¼”ç¤º
        demo.run_comprehensive_demo()

        print("\n" + "=" * 80)
        print("âœ… æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ä½¿ç”¨æŒ‡å—:")
        print("1. è§†é¢‘ç´ æ: æ”¯æŒå›¾åƒåºåˆ—æ¨¡æ‹Ÿè§†é¢‘")
        print("2. æ–‡æœ¬ç´ æ: å‡†å¤‡æè¿°æ€§æ–‡æœ¬åˆ—è¡¨")
        print("3. å¤„ç†æµç¨‹: å›¾åƒâ†’ç‰¹å¾ç¼–ç â†’æ–‡æœ¬åŒ¹é…")
        print("4. åº”ç”¨åœºæ™¯: è§†é¢‘æ£€ç´¢ã€å†…å®¹ç†è§£ã€è‡ªåŠ¨æ ‡æ³¨")

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()