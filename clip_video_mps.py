import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# å¿½ç•¥NumPyè­¦å‘Š
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)

import torch
import torch.nn.functional as F
from PIL import Image, ImageDraw
import numpy as np
from pathlib import Path
import json
import cv2

# ç›´æ¥å¯¼å…¥æ ¸å¿ƒç»„ä»¶ï¼Œç»•è¿‡image_processingé—®é¢˜
def load_clip_directly():
    """ç›´æ¥åŠ è½½CLIPæ¨¡å‹ï¼Œç»•è¿‡image_processingé—®é¢˜"""
    try:
        # ç›´æ¥ä»modelingæ¨¡å—å¯¼å…¥ï¼Œé¿å…processingé—®é¢˜
        from transformers.models.clip.modeling_clip import CLIPModel
        from transformers.models.clip.configuration_clip import CLIPConfig
        from transformers import AutoTokenizer

        print("âœ… æˆåŠŸå¯¼å…¥æ ¸å¿ƒCLIPç»„ä»¶")
        return CLIPModel, CLIPConfig, AutoTokenizer, True
    except ImportError as e:
        print(f"âŒ æ ¸å¿ƒç»„ä»¶å¯¼å…¥å¤±è´¥: {e}")
        return None, None, None, False

# åˆ›å»ºç®€åŒ–çš„å¤„ç†å™¨
class SimpleProcessor:
    """ç®€åŒ–çš„CLIPå¤„ç†å™¨ï¼Œé¿å…image_processingé—®é¢˜"""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, text=None, images=None, return_tensors="pt", padding=True):
        # å¤„ç†æ–‡æœ¬
        if text is not None:
            if isinstance(text, str):
                text = [text]

            # ä½¿ç”¨tokenizerå¤„ç†æ–‡æœ¬
            text_inputs = self.tokenizer(
                text,
                return_tensors=return_tensors,
                padding=padding,
                truncation=True,
                max_length=77
            )
        else:
            text_inputs = {}

        # å¤„ç†å›¾åƒ
        if images is not None:
            if isinstance(images, Image.Image):
                images = [images]

            # ç®€å•çš„å›¾åƒé¢„å¤„ç†
            pixel_values = []
            for img in images:
                # è°ƒæ•´å¤§å°åˆ°224x224
                img = img.resize((224, 224))
                # è½¬æ¢ä¸ºtensor
                img_array = torch.tensor(list(img.getdata())).float()
                img_array = img_array.view(224, 224, 3)
                # è½¬æ¢ä¸ºCHWæ ¼å¼å¹¶å½’ä¸€åŒ–
                img_tensor = img_array.permute(2, 0, 1) / 255.0
                # æ ‡å‡†åŒ– (ImageNetæ ‡å‡†)
                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                img_tensor = (img_tensor - mean) / std
                pixel_values.append(img_tensor)

            pixel_values = torch.stack(pixel_values)
            text_inputs['pixel_values'] = pixel_values

        return text_inputs

# å°è¯•å¯¼å…¥ç»„ä»¶
CLIPModel, CLIPConfig, AutoTokenizer, clip_available = load_clip_directly()

# è®¾ç½®è®¾å¤‡ - ä¼˜å…ˆä½¿ç”¨MPS
def get_device():
    if torch.backends.mps.is_available():
        return torch.device("mps")
    elif torch.cuda.is_available():
        return torch.device("cuda")
    else:
        return torch.device("cpu")

device = get_device()
print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")

class VideoProcessor:
    """è§†é¢‘å¤„ç†å·¥å…·ç±» - åˆ›å»ºè§†é¢‘å¸§åºåˆ—å¹¶ç”ŸæˆMP4"""

    @staticmethod
    def create_sample_video_frames(output_dir="./test_materials", num_frames=16):
        """åˆ›å»ºç¤ºä¾‹è§†é¢‘å¸§åºåˆ—"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        frames = []
        frame_descriptions = []
        full_size_frames = []  # ä¿å­˜å®Œæ•´å°ºå¯¸çš„å¸§ç”¨äºè§†é¢‘ç”Ÿæˆ

        print(f"ğŸ¬ åˆ›å»º {num_frames} ä¸ªè§†é¢‘å¸§...")

        for i in range(num_frames):
            # åˆ›å»º640x480çš„å›¾åƒ
            img = Image.new('RGB', (640, 480), color=(30, 30, 30))
            draw = ImageDraw.Draw(img)

            # åœºæ™¯1: ç§»åŠ¨çš„åœ†å½¢ (å‰8å¸§)
            if i < 8:
                # æ·»åŠ ç§»åŠ¨çš„é»„è‰²åœ†å½¢
                center_x = int(100 + 400 * (i / 7))
                center_y = int(240 + 100 * np.sin(2 * np.pi * i / 8))

                radius = 40
                draw.ellipse([
                    center_x - radius, center_y - radius,
                    center_x + radius, center_y + radius
                ], fill=(255, 255, 0))

                # æ·»åŠ æ–‡å­—æè¿°
                frame_text = f"Moving Ball - Frame {i+1}"
                draw.text((50, 50), frame_text, fill=(255, 255, 255))

                # æ·»åŠ åœºæ™¯æè¿°
                scene_desc = f"Scene 1: Yellow ball moving from left to right"
                draw.text((50, 420), scene_desc, fill=(200, 200, 200))

                frame_descriptions.append("a moving yellow ball")

            # åœºæ™¯2: è·³è·ƒçš„æ–¹å— (å8å¸§)
            else:
                frame_in_scene = i - 8
                # æ·»åŠ è·³è·ƒçš„çº¢è‰²æ–¹å—
                center_x = 320
                # æŠ›ç‰©çº¿è¿åŠ¨
                center_y = int(400 - 200 * (4 * frame_in_scene * (7 - frame_in_scene)) / 49)

                size = 60
                draw.rectangle([
                    center_x - size//2, center_y - size//2,
                    center_x + size//2, center_y + size//2
                ], fill=(255, 100, 100))

                # æ·»åŠ æ–‡å­—æè¿°
                frame_text = f"Jumping Box - Frame {i+1}"
                draw.text((50, 50), frame_text, fill=(255, 255, 255))

                # æ·»åŠ åœºæ™¯æè¿°
                scene_desc = f"Scene 2: Red box jumping up and down"
                draw.text((50, 420), scene_desc, fill=(200, 200, 200))

                frame_descriptions.append("a jumping red box")

            # ä¿å­˜å®Œæ•´å°ºå¯¸çš„å¸§ç”¨äºè§†é¢‘ç”Ÿæˆ
            full_size_frames.append(img.copy())

            # è°ƒæ•´å¤§å°åˆ°224x224ï¼ˆCLIPæ ‡å‡†è¾“å…¥å°ºå¯¸ï¼‰
            img_resized = img.resize((224, 224))
            frames.append(img_resized)

            # ä¿å­˜åŸå§‹å¤§å°çš„å›¾åƒæ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            img_path = output_path / f"frame_{i:03d}.jpg"
            img.save(img_path)

        print(f"âœ… åˆ›å»ºäº† {num_frames} ä¸ªè§†é¢‘å¸§")
        return frames, frame_descriptions, full_size_frames

    @staticmethod
    def create_video_from_frames(full_size_frames, output_path="./videos/clip_test_1.mp4", fps=2):
        """ä»å¸§åºåˆ—åˆ›å»ºMP4è§†é¢‘"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(output_path).parent
        output_dir.mkdir(exist_ok=True)

        print(f"ğŸ¥ åˆ›å»ºè§†é¢‘: {output_path}")

        if not full_size_frames:
            print("âŒ æ²¡æœ‰å¸§æ•°æ®")
            return False

        # è½¬æ¢PILå›¾åƒä¸ºOpenCVæ ¼å¼
        cv_frames = []
        for pil_frame in full_size_frames:
            # è½¬æ¢PILå›¾åƒä¸ºnumpyæ•°ç»„
            frame_array = np.array(pil_frame)
            # è½¬æ¢RGBåˆ°BGRï¼ˆOpenCVæ ¼å¼ï¼‰
            frame_bgr = cv2.cvtColor(frame_array, cv2.COLOR_RGB2BGR)
            cv_frames.append(frame_bgr)

        # è·å–å¸§å°ºå¯¸
        height, width, layers = cv_frames[0].shape

        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))

        # å†™å…¥æ‰€æœ‰å¸§
        for frame in cv_frames:
            video_writer.write(frame)

        video_writer.release()
        print(f"âœ… è§†é¢‘å·²ä¿å­˜: {output_path}")
        return True

    @staticmethod
    def create_analysis_video(frames, texts, results, output_path="./videos/clip_analysis.mp4", fps=2):
        """åˆ›å»ºå¸¦æœ‰åˆ†æç»“æœçš„è§†é¢‘"""
        output_dir = Path(output_path).parent
        output_dir.mkdir(exist_ok=True)

        print(f"ğŸ“Š åˆ›å»ºåˆ†æè§†é¢‘: {output_path}")

        # åˆ›å»ºå¸¦åˆ†æç»“æœçš„å¸§
        analysis_frames = []

        for i, (frame, frame_results) in enumerate(zip(frames, results)):
            # æ”¾å¤§å¸§åˆ°640x480ç”¨äºæ˜¾ç¤º
            display_frame = frame.resize((640, 480))
            draw = ImageDraw.Draw(display_frame)

            # æ·»åŠ å¸§å·
            draw.text((10, 10), f"Frame {i+1}", fill=(255, 255, 255))

            # æ·»åŠ åˆ†æç»“æœ
            y_offset = 50
            for j, (text, score) in enumerate(zip(texts, frame_results)):
                score_val = score.item()
                color = (255, 255, 255) if score_val > 0.3 else (150, 150, 150)

                # ç»˜åˆ¶æ–‡æœ¬å’Œåˆ†æ•°
                result_text = f"{text}: {score_val:.3f}"
                draw.text((10, y_offset + j * 25), result_text, fill=color)

                # ç»˜åˆ¶åˆ†æ•°æ¡
                bar_width = int(score_val * 200)
                if bar_width > 0:
                    draw.rectangle([
                        250, y_offset + j * 25 + 5,
                        250 + bar_width, y_offset + j * 25 + 15
                    ], fill=(0, 255, 0) if score_val > 0.3 else (100, 100, 100))

            # è½¬æ¢ä¸ºOpenCVæ ¼å¼
            frame_array = np.array(display_frame)
            frame_bgr = cv2.cvtColor(frame_array, cv2.COLOR_RGB2BGR)
            analysis_frames.append(frame_bgr)

        # åˆ›å»ºè§†é¢‘
        height, width, layers = analysis_frames[0].shape
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))

        for frame in analysis_frames:
            video_writer.write(frame)

        video_writer.release()
        print(f"âœ… åˆ†æè§†é¢‘å·²ä¿å­˜: {output_path}")
        return True

class CLIPVideoDemo:
    """CLIPè§†é¢‘å¤„ç†æ¼”ç¤ºç±»"""

    def __init__(self):
        print("ğŸ”„ åˆå§‹åŒ–CLIPè§†é¢‘æ¼”ç¤º...")

        if not clip_available:
            print("âŒ CLIPç»„ä»¶ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
            self._create_demo_model()
            return

        # åŠ è½½CLIPæ¨¡å‹
        try:
            print("ğŸ“¦ åŠ è½½CLIPæ¨¡å‹...")

            # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶
            model_dir = Path("./models")
            possible_paths = [
                model_dir / "models--openai--clip-vit-base-patch32" / "snapshots",
                model_dir / "clip-model",
                model_dir
            ]

            local_model_path = None
            for path in possible_paths:
                if path.exists():
                    # æŸ¥æ‰¾åŒ…å«config.jsonçš„å­ç›®å½•
                    for subdir in path.rglob("*"):
                        if subdir.is_dir() and (subdir / "config.json").exists():
                            local_model_path = subdir
                            break
                    if local_model_path:
                        break

            if local_model_path:
                print(f"ğŸ“ æ‰¾åˆ°æœ¬åœ°æ¨¡å‹æ–‡ä»¶: {local_model_path}")

                # åŠ è½½é…ç½®
                config_path = local_model_path / "config.json"
                with open(config_path, 'r') as f:
                    config_dict = json.load(f)
                config = CLIPConfig.from_dict(config_dict)

                # åŠ è½½æ¨¡å‹
                self.model = CLIPModel.from_pretrained(
                    str(local_model_path),
                    config=config,
                    local_files_only=True
                ).to(device)

                # åŠ è½½tokenizer
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        str(local_model_path),
                        local_files_only=True
                    )
                except:
                    # å¦‚æœæœ¬åœ°tokenizeråŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åœ¨çº¿ç‰ˆæœ¬
                    print("âš ï¸  æœ¬åœ°tokenizeråŠ è½½å¤±è´¥ï¼Œä½¿ç”¨åœ¨çº¿ç‰ˆæœ¬...")
                    self.tokenizer = AutoTokenizer.from_pretrained("openai/clip-vit-base-patch32")

                # åˆ›å»ºç®€åŒ–å¤„ç†å™¨
                self.processor = SimpleProcessor(self.tokenizer)

                print(f"âœ… CLIPæ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¿è¡Œåœ¨ {device}")
                self.model_loaded = True

                # æµ‹è¯•æ¨¡å‹
                self._test_model()

            else:
                print("âŒ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹æ–‡ä»¶")
                raise Exception("æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")

        except Exception as e:
            print(f"âŒ CLIPæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”§ ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼...")
            self.model_loaded = False
            self._create_demo_model()

        self.video_processor = VideoProcessor()

    def _test_model(self):
        """æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
        try:
            print("ğŸ§ª æµ‹è¯•æ¨¡å‹åŠŸèƒ½...")

            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_image = Image.new('RGB', (224, 224), color='red')
            test_texts = ["a red image", "a blue image"]

            # å¤„ç†è¾“å…¥
            inputs = self.processor(text=test_texts, images=test_image)

            # å°†è¾“å…¥ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                     for k, v in inputs.items()}

            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)

            print("âœ… æ¨¡å‹æµ‹è¯•æˆåŠŸï¼")

        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")

    def _create_demo_model(self):
        """åˆ›å»ºæ¼”ç¤ºæ¨¡å‹"""
        class DemoModel:
            def __call__(self, **kwargs):
                batch_size = kwargs.get('pixel_values', torch.randn(1, 3, 224, 224)).shape[0]
                text_batch_size = kwargs.get('input_ids', torch.randint(0, 1000, (1, 77))).shape[0]

                return type('Output', (), {
                    'image_embeds': torch.randn(batch_size, 512),
                    'text_embeds': torch.randn(text_batch_size, 512),
                    'logits_per_image': torch.randn(batch_size, text_batch_size),
                    'logits_per_text': torch.randn(text_batch_size, batch_size)
                })()

        class DemoProcessor:
            def __call__(self, images=None, text=None, return_tensors="pt", padding=True):
                batch_size = len(text) if text else 1
                return {
                    'pixel_values': torch.randn(1, 3, 224, 224),
                    'input_ids': torch.randint(0, 1000, (batch_size, 77)),
                    'attention_mask': torch.ones(batch_size, 77)
                }

        self.model = DemoModel()
        self.processor = DemoProcessor()
        self.model_loaded = False
        print("âœ… æ¼”ç¤ºæ¨¡å‹åˆ›å»ºå®Œæˆ")

    def analyze_single_frame(self, frame, texts):
        """åˆ†æå•ä¸ªè§†é¢‘å¸§"""
        inputs = self.processor(
            images=frame,
            text=texts,
            return_tensors="pt",
            padding=True
        )

        if self.model_loaded:
            # å°†è¾“å…¥ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                     for k, v in inputs.items()}

        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = F.softmax(outputs.logits_per_image, dim=-1)

        return probs.cpu() if self.model_loaded else probs

    def analyze_video_sequence(self, frames, texts):
        """åˆ†ææ•´ä¸ªè§†é¢‘åºåˆ—"""
        print(f"ğŸï¸ åˆ†æ {len(frames)} ä¸ªè§†é¢‘å¸§...")

        frame_results = []

        for i, frame in enumerate(frames):
            print(f"   å¤„ç†å¸§ {i+1}/{len(frames)}")
            probs = self.analyze_single_frame(frame, texts)
            frame_results.append(probs[0])  # å–ç¬¬ä¸€ä¸ªå›¾åƒçš„ç»“æœ

        return torch.stack(frame_results)

    def create_comprehensive_video_demo(self):
        """åˆ›å»ºå®Œæ•´çš„è§†é¢‘æ¼”ç¤º"""
        print("\nğŸ¬ åˆ›å»ºå®Œæ•´çš„CLIPè§†é¢‘æ¼”ç¤º")
        print("=" * 60)

        # åˆ›å»ºè§†é¢‘å¸§å’Œæè¿°
        frames, frame_descriptions, full_size_frames = self.video_processor.create_sample_video_frames()

        # åˆ›å»ºåŸºç¡€è§†é¢‘ - ä½¿ç”¨å®Œæ•´å°ºå¯¸çš„å¸§
        self.video_processor.create_video_from_frames(full_size_frames, "./videos/clip_test_1.mp4", fps=2)

        # å®šä¹‰å€™é€‰æ–‡æœ¬
        texts = [
            "a moving yellow ball",
            "a jumping red box",
            "animated graphics",
            "computer animation",
            "geometric shapes"
        ]

        print(f"\nğŸ“ åˆ†ææ–‡æœ¬: {texts}")

        # åˆ†æè§†é¢‘åºåˆ—ï¼ˆä½¿ç”¨224x224çš„å¸§ï¼‰
        results = self.analyze_video_sequence(frames, texts)

        # åˆ›å»ºåˆ†æè§†é¢‘
        self.video_processor.create_analysis_video(frames, texts, results, "./videos/clip_analysis.mp4", fps=2)

        # æ˜¾ç¤ºè¯¦ç»†åˆ†æç»“æœ
        print(f"\nğŸ“Š è¯¦ç»†åˆ†æç»“æœ:")
        print("=" * 80)

        # åˆ›å»ºè§†é¢‘æè¿°æ–‡ä»¶
        description_file = Path("./videos/video_description.txt")
        with open(description_file, 'w', encoding='utf-8') as f:
            f.write("CLIPè§†é¢‘åˆ†æç»“æœ\n")
            f.write("=" * 50 + "\n\n")
            f.write("è§†é¢‘æ–‡ä»¶: clip_test_1.mp4\n")
            f.write("åˆ†æè§†é¢‘: clip_analysis.mp4\n\n")
            f.write("è§†é¢‘å†…å®¹æè¿°:\n")
            f.write("- å‰8å¸§: é»„è‰²åœ†çƒä»å·¦åˆ°å³ç§»åŠ¨\n")
            f.write("- å8å¸§: çº¢è‰²æ–¹å—ä¸Šä¸‹è·³è·ƒ\n\n")
            f.write("å€™é€‰æ–‡æœ¬:\n")
            for i, text in enumerate(texts, 1):
                f.write(f"{i}. {text}\n")
            f.write("\n")

            # é€å¸§åˆ†æç»“æœ
            f.write("é€å¸§åˆ†æç»“æœ:\n")
            f.write("-" * 30 + "\n")
            for i, frame_probs in enumerate(results):
                f.write(f"\nå¸§ {i+1:2d} ({frame_descriptions[i]}):\n")
                for j, text in enumerate(texts):
                    score = frame_probs[j].item()
                    f.write(f"  {text:25s}: {score:.4f}\n")

            # å¹³å‡åˆ†æ•°
            f.write(f"\næ•´ä½“å¹³å‡åˆ†æ•°:\n")
            f.write("-" * 20 + "\n")
            avg_scores = results.mean(dim=0)
            for i, text in enumerate(texts):
                score = avg_scores[i].item()
                f.write(f"{text:25s}: {score:.4f}\n")

        # åœ¨æ§åˆ¶å°æ˜¾ç¤ºæ‘˜è¦
        print("ğŸ“‹ è§†é¢‘å†…å®¹æ‘˜è¦:")
        print("  ğŸ¯ åœºæ™¯1 (å¸§1-8): é»„è‰²åœ†çƒä»å·¦åˆ°å³ç§»åŠ¨")
        print("  ğŸ¯ åœºæ™¯2 (å¸§9-16): çº¢è‰²æ–¹å—ä¸Šä¸‹è·³è·ƒ")
        print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  ğŸ¥ åŸå§‹è§†é¢‘: ./videos/clip_test_1.mp4")
        print(f"  ğŸ“Š åˆ†æè§†é¢‘: ./videos/clip_analysis.mp4")
        print(f"  ğŸ“ è¯¦ç»†æŠ¥å‘Š: ./videos/video_description.txt")

        # æ˜¾ç¤ºæœ€ä½³åŒ¹é…
        print(f"\nğŸ† æœ€ä½³æ–‡æœ¬åŒ¹é…:")
        avg_scores = results.mean(dim=0)
        best_idx = torch.argmax(avg_scores)
        best_text = texts[best_idx]
        best_score = avg_scores[best_idx].item()
        print(f"  '{best_text}': {best_score:.4f}")

        # åœºæ™¯åˆ‡æ¢åˆ†æ
        scene1_scores = results[:8].mean(dim=0)  # å‰8å¸§
        scene2_scores = results[8:].mean(dim=0)  # å8å¸§

        print(f"\nğŸ¬ åœºæ™¯åˆ†æ:")
        print(f"  åœºæ™¯1 (ç§»åŠ¨çƒ) æœ€ä½³åŒ¹é…: '{texts[torch.argmax(scene1_scores)]}' ({scene1_scores.max():.4f})")
        print(f"  åœºæ™¯2 (è·³è·ƒç›’) æœ€ä½³åŒ¹é…: '{texts[torch.argmax(scene2_scores)]}' ({scene2_scores.max():.4f})")

        return True

    def run_comprehensive_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸ¬ CLIPè§†é¢‘å¤„ç†ç»¼åˆæ¼”ç¤º")
        print(f"ğŸ”§ è¿è¡Œè®¾å¤‡: {device}")
        if self.model_loaded:
            print("ğŸ‰ ä½¿ç”¨çœŸå®CLIPæ¨¡å‹")
        else:
            print("ğŸ­ ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼")
        print("=" * 80)

        # åˆ›å»ºè§†é¢‘æ¼”ç¤º
        self.create_comprehensive_video_demo()

        print("\n" + "=" * 80)
        print("âœ… CLIPè§†é¢‘å¤„ç†æ¼”ç¤ºå®Œæˆï¼")
        print(f"ğŸ’¡ ä½¿ç”¨äº† {device} è¿›è¡ŒåŠ é€Ÿè®¡ç®—")
        if self.model_loaded:
            print("ğŸ‰ æˆåŠŸä½¿ç”¨çœŸå®CLIPæ¨¡å‹è¿›è¡Œæ¨ç†")
        else:
            print("ğŸ­ ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼å®ŒæˆåŠŸèƒ½å±•ç¤º")

        print("\nğŸ“º æŸ¥çœ‹ç»“æœ:")
        print("  1. æ‰“å¼€ ./videos/clip_test_1.mp4 æŸ¥çœ‹åŸå§‹è§†é¢‘")
        print("  2. æ‰“å¼€ ./videos/clip_analysis.mp4 æŸ¥çœ‹åˆ†æç»“æœ")
        print("  3. æŸ¥çœ‹ ./videos/video_description.txt äº†è§£è¯¦ç»†åˆ†æ")
        print("=" * 80)

def show_video_papers():
    """æ˜¾ç¤ºè§†é¢‘ç›¸å…³çš„SOTAè®ºæ–‡"""
    print("\nğŸ“š è§†é¢‘-æ–‡å­—å¤„ç†SOTAæ¨¡å‹è®ºæ–‡:")
    print("=" * 80)

    papers = [
        {
            "name": "CLIP",
            "paper": "Learning Transferable Visual Models From Natural Language Supervision",
            "url": "https://arxiv.org/abs/2103.00020",
            "description": "OpenAIçš„åŸå§‹CLIPæ¨¡å‹ï¼Œå¯ç”¨äºè§†é¢‘å¸§ç†è§£"
        },
        {
            "name": "VideoCLIP",
            "paper": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding",
            "url": "https://arxiv.org/abs/2109.14084",
            "description": "ä¸“é—¨é’ˆå¯¹è§†é¢‘ç†è§£çš„CLIPæ‰©å±•"
        },
        {
            "name": "X-CLIP",
            "paper": "Expanding Language-Image Pretrained Models for General Video Recognition",
            "url": "https://arxiv.org/abs/2208.02816",
            "description": "å¾®è½¯æå‡ºçš„è·¨æ¨¡æ€è§†é¢‘ç†è§£æ¨¡å‹"
        },
        {
            "name": "Video-ChatGPT",
            "paper": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
            "url": "https://arxiv.org/abs/2306.05424",
            "description": "ç»“åˆChatGPTçš„è§†é¢‘ç†è§£æ¨¡å‹"
        }
    ]

    for i, paper in enumerate(papers, 1):
        print(f"\n{i}. **{paper['name']}**")
        print(f"   ğŸ“„ {paper['paper']}")
        print(f"   ğŸ”— {paper['url']}")
        print(f"   ğŸ’¡ {paper['description']}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ åŸºäºCLIPçš„è§†é¢‘-æ–‡å­—å¤„ç†æ¼”ç¤º")
    print(f"ğŸš€ æ”¯æŒApple Silicon MPSåŠ é€Ÿ")
    print("ğŸ”§ ç»•è¿‡NumPyå…¼å®¹æ€§é—®é¢˜")
    print("ğŸ¥ ç”Ÿæˆå¯è§†åŒ–è§†é¢‘æ–‡ä»¶")
    print("=" * 80)

    # æ˜¾ç¤ºç›¸å…³è®ºæ–‡
    show_video_papers()

    try:
        # åˆ›å»ºå¹¶è¿è¡Œæ¼”ç¤º
        demo = CLIPVideoDemo()
        demo.run_comprehensive_demo()

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

